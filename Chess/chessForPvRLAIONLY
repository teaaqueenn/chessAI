import chess
import tkinter as tk
from tkinter import messagebox
import threading
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random
import time
import matplotlib as plt
from matplotlib import figure
from matplotlib import pyplot
import copy



GREEN = f'#395631'
BEIGE = f'#d1b281'

global game_numbers
game_numbers = []
total_rewards = []
losses = []


# Initialize the chess board
board = chess.Board()

# Colors for the chess pieces
white_pieces = {
    chess.PAWN: "♙", chess.KNIGHT: "♘", chess.BISHOP: "♗", chess.ROOK: "♖",
    chess.QUEEN: "♕", chess.KING: "♔"
}
black_pieces = {
    chess.PAWN: "♟", chess.KNIGHT: "♞", chess.BISHOP: "♝", chess.ROOK: "♜",
    chess.QUEEN: "♛", chess.KING: "♚"
}

# Tkinter setup
root = tk.Tk()
root.title("Chess Game")

# Create a canvas to draw the chessboard
canvas = tk.Canvas(root, width=400, height=400)
canvas.pack()

# Define the Deep Q-Network (DQN) model
import torch
import torch.nn as nn

class DQN(nn.Module):
    def __init__(self):
        super(DQN, self).__init__()
        
        # Input layer: 12 * 8 * 8 = 768 (flattened input)
        self.fc1 = nn.Linear(12 * 8 * 8, 2048)  # First hidden layer with 2048 units
        self.fc2 = nn.Linear(2048, 2048)        # Second hidden layer with 2048 units
        self.fc3 = nn.Linear(2048, 1024)        # Third hidden layer with 1024 units
        self.fc4 = nn.Linear(1024, 1024)        # Fourth hidden layer with 1024 units
        self.fc5 = nn.Linear(1024, 512)         # Fifth hidden layer with 512 units
        self.fc6 = nn.Linear(512, 512)          # Sixth hidden layer with 512 units
        self.fc7 = nn.Linear(512, 256)          # Seventh hidden layer with 256 units
        self.fc8 = nn.Linear(256, 256)          # Eighth hidden layer with 256 units
        self.fc9 = nn.Linear(256, 128)          # Ninth hidden layer with 128 units
        self.fc10 = nn.Linear(128, 128)         # Tenth hidden layer with 128 units
        self.fc11 = nn.Linear(128, 64)          # Eleventh hidden layer with 64 units
        self.fc12 = nn.Linear(64, 64)           # Twelfth hidden layer with 64 units
        self.fc13 = nn.Linear(64, 32)           # Thirteenth hidden layer with 32 units
        self.fc14 = nn.Linear(32, 32)           # Fourteenth hidden layer with 32 units
        self.fc15 = nn.Linear(32, 16)           # Fifteenth hidden layer with 16 units
        self.fc16 = nn.Linear(16, 16)           # Sixteenth hidden layer with 16 units
        self.fc17 = nn.Linear(16, 8)            # Seventeenth hidden layer with 8 units
        self.fc18 = nn.Linear(8, 8)             # Eighteenth hidden layer with 8 units
        self.fc19 = nn.Linear(8, 4)             # Nineteenth hidden layer with 4 units
        self.fc20 = nn.Linear(4, 4672)          # Twentieth hidden layer (output) with 4672 units (for possible moves)

        # Dropout layer to prevent overfitting (applied after each hidden layer)
        self.dropout = nn.Dropout(0.3)  # Dropout with 50% probability of zeroing out inputs

    def forward(self, x):
        # Passing the input through all the layers with ReLU activation
        x = torch.relu(self.fc1(x))
        x = self.dropout(x)
        
        x = torch.relu(self.fc2(x))
        x = self.dropout(x)

        x = torch.relu(self.fc3(x))
        x = self.dropout(x)

        x = torch.relu(self.fc4(x))
        x = self.dropout(x)

        x = torch.relu(self.fc5(x))
        x = self.dropout(x)

        x = torch.relu(self.fc6(x))
        x = self.dropout(x)

        x = torch.relu(self.fc7(x))
        x = self.dropout(x)

        x = torch.relu(self.fc8(x))
        x = self.dropout(x)

        x = torch.relu(self.fc9(x))
        x = self.dropout(x)

        x = torch.relu(self.fc10(x))
        x = self.dropout(x)

        x = torch.relu(self.fc11(x))
        x = self.dropout(x)

        x = torch.relu(self.fc12(x))
        x = self.dropout(x)

        x = torch.relu(self.fc13(x))
        x = self.dropout(x)

        x = torch.relu(self.fc14(x))
        x = self.dropout(x)

        x = torch.relu(self.fc15(x))
        x = self.dropout(x)

        x = torch.relu(self.fc16(x))
        x = self.dropout(x)

        x = torch.relu(self.fc17(x))
        x = self.dropout(x)

        x = torch.relu(self.fc18(x))
        x = self.dropout(x)

        x = torch.relu(self.fc19(x))
        x = self.dropout(x)

        # Output layer (no activation function here, raw Q-values)
        x = self.fc20(x)

        return x


class ChessRLAI:
    def __init__(self, model = DQN(), epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995, learning_rate=0.001, gamma=0.99):
        self.model = model
        self.epsilon = epsilon  # Initial epsilon
        self.epsilon_min = epsilon_min  # Minimum value of epsilon
        self.epsilon_decay = epsilon_decay  # Decay factor
        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
        self.loss_fn = nn.MSELoss()  # Loss function for Q-values
        self.gamma = gamma  # Discount factor for future rewards

        self.total_reward = 0.0
        self.total_loss = 0.0


    def reward_of_move(self, move, board) -> float:
        """
        Calculate the reward for a given move. The reward is based on:
        - Material gain or loss
        - Whether the move puts the opponent in check or checkmate
        - 

        Parameters:
            move (chess.Move): The move to evaluate.
            board (chess.Board): The current board state.

        Returns:
            float: The reward for the move.
        """
        reward = 0.0

        if board.is_capture(move) and move is not None:
            captured_piece = board.piece_at(move.to_square)
            # Assign positive reward for capturing a piece
            if captured_piece:
                piece_type = captured_piece.piece_type
                # Assign value based on the piece captured
                if piece_type == chess.PAWN:
                    reward += 1.0
                elif piece_type == piece_type == chess.BISHOP:
                    reward += 3.0
                elif piece_type == piece_type == chess.BISHOP:
                    reward += 5.0
                elif piece_type == chess.ROOK:
                    reward += 7.0
                elif piece_type == chess.QUEEN:
                    reward += 9.0

        # Step 2: Check if the move results in check or checkmate
        board.push(move)
        if board.is_checkmate():
            reward += 50  # Positive large reward for checkmate
        elif board.is_check():
            reward += 10  # Smaller positive reward for check
        board.pop()

        return reward

    def find_best_move_with_q_values(self, board, gamma=0.99, alpha=0.1):
        """
        Calculate the Q-values for each legal move, update the Q-value using Q-learning,
        and return the move with the highest Q-value.

        Parameters:
            board (chess.Board): The current state of the board.
            legal_moves (list): A list of legal moves (chess.Move objects).
            reward (float): The reward for making the move.
            gamma (float): The discount factor for future rewards.
            alpha (float): The learning rate for updating Q-values.

        Returns:
            chess.Move: The move with the highest Q-value (in UCI notation).
        """
        # Initialize a list to store the Q-values for each legal move
        q_values = []

        legal_moves = list(board.legal_moves)

        rand_val = random.random()

        print("rand val: ", rand_val)
        print("epsilon val: ", self.epsilon)

        # Initialize best_move outside the if-else block
        best_move = None

        if rand_val < self.epsilon:
            # Explore: Choose a random legal move
            best_move = random.choice(legal_moves)
        else:
            print("using q-vals")

            # Exploit: Choose the move with the highest Q-value from the model's output
            with torch.no_grad():
                # Loop through each legal move and calculate the Q-value
                for move in legal_moves:
                    # Step 1: Convert the current board state to tensor
                    state_tensor = board_to_tensor(board).unsqueeze(0)  # Add batch dimension

                    # Step 2: Get the Q-values for the current state
                    current_q_values = self.model(state_tensor).squeeze(0)

                    # Step 3: Simulate the move by copying the board and making the move on the clone
                    cloned_board = copy.deepcopy(board)  # Clone the board
                    cloned_board.push(move)  # Make the move on the cloned board

                    # Step 4: Convert the new state after the move to tensor
                    next_state_tensor = board_to_tensor(cloned_board).unsqueeze(0)  # Get the next state tensor after the move

                    # Step 5: Get the Q-values for the next state
                    next_q_values = self.model(next_state_tensor).squeeze(0)

                    # Step 6: Calculate the maximum Q-value for the next state
                    max_next_q_value = torch.max(next_q_values).item()

                    # Step 7: Find the index of the move in the model's Q-value predictions
                    move_index = legal_moves.index(move)

                    # Step 8: Get the current Q-value for the move
                    current_q_value = current_q_values[move_index].item()

                    # Step 9: Calculate the new Q-value using the Q-learning equation
                    updated_q_value = current_q_value + alpha * (self.reward_of_move(move, board) + gamma * max_next_q_value - current_q_value)

                    # Append the updated Q-value to the list
                    q_values.append(updated_q_value)

                # Convert q_values list to tensor
                q_values_tensor = torch.tensor(q_values)

                # Step 10: Calculate the loss (using MSE) and update Q-value
                # We should update the Q-value for the move that was selected
                updated_q_value_tensor = torch.tensor([updated_q_value])  # Convert updated_q_value to tensor

                # Ensure q_values_tensor is a tensor of shape (N,)
                loss = self.loss_fn(q_values_tensor, updated_q_value_tensor)

                # Update the total loss
                self.total_loss += loss.item()

                # Step 11: Find the move with the highest Q-value (from q_values)
                best_move_index = np.argmax(q_values)
                best_move = legal_moves[best_move_index]

                # Track cumulative reward (this should be done only after best_move is assigned)
                self.total_reward += self.reward_of_move(best_move, board)

        # Decay epsilon after each move
        self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)

        return best_move


def plot_results():
    global white_wins, black_wins, stalemates, game_numbers, total_rewards, losses

    # Create a 1x2 grid of subplots (1 row, 2 columns)
    fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(14, 6))  # Adjusted figsize to fit two subplots

    # Plot Total Reward vs Game Number (First subplot - Left)
    axs[0].plot(game_numbers, total_rewards, color='b', label='Total Rewards')  # Line plot for total rewards
    axs[0].scatter(game_numbers, losses, color='r', label='Losses')  # Scatter plot for losses
    axs[0].set(xlabel='Game Number', ylabel='Total Reward / Losses', title='Game Number vs Total Reward and Losses')
    axs[0].grid(True)
    axs[0].legend()  # Show legend for both line and scatter

    # Pie chart (Second subplot - Right)
    axs[1].pie([white_wins, black_wins, stalemates], labels=['White Wins', 'Black Wins', 'Stalemates'],
               autopct='%1.1f%%', startangle=90)
    axs[1].set_title('Game Results')

    # Adjust layout to make sure there's no overlap
    plt.tight_layout()

    # Show the plot
    plt.show()


# Function to display the board in Tkinter
def display_board():
    # Clear the canvas
    
    canvas.delete("all")
    square_size = 50
    for row in range(8):
        for col in range(8):
            # Determine the color of the square
            color = GREEN if (row + col) % 2 == 0 else BEIGE
            canvas.create_rectangle(col * square_size, row * square_size,
                                    (col + 1) * square_size, (row + 1) * square_size,
                                    fill=color)
            
            # Get the piece at the current square
            piece = board.piece_at(chess.square(col, 7 - row))  # Flip y-axis for correct orientation
            if piece:
                piece_char = ""
                if piece.color == chess.WHITE:
                    piece_char = white_pieces.get(piece.piece_type, "")
                else:
                    piece_char = black_pieces.get(piece.piece_type, "")
                canvas.create_text(col * square_size + square_size / 2,
                                   row * square_size + square_size / 2,
                                   text=piece_char, font=("Arial", 24))

    # Draw row labels (1-8) on the left side of the board
    for row in range(8):
        canvas.create_text(-10, row * square_size + square_size / 2,
                           text=str(8 - row), font=("Arial", 14))  # 8-1, 7-2, ..., 1-8

    # Draw column labels (a-h) at the bottom of the board
    for col in range(8):
        canvas.create_text(col * square_size + square_size / 2,
                           8 * square_size + 10,  # Position below the board
                           text=chr(ord('a') + col), font=("Arial", 14))
    
    plot_results()


def display_title():
    # Clear the canvas
    canvas.delete("all")
    
    # Add the title text at the top of the canvas
    canvas.create_text(200, 50, text="Welcome to Chess!", font=("Arial", 24, "bold"), fill="black")

    # Add a description text below the title
    description = (
        "Choose a game mode to start: \n\n"
        "1. Player vs Player\n"
        "2. Player vs RLAI\n"
        "3. RLAI vs RLAI"
    )
    canvas.create_text(200, 150, text=description, font=("Arial", 14), fill="black", justify="center")
    
    # Ensure canvas updates
    root.update()  # This forces an update of the canvas and GUI elements


# Function to handle a player's move from the console
def make_move(move_uci):
    global board
    try:
        # Apply the move using UCI notation
        move = chess.Move.from_uci(move_uci)
        if move in board.legal_moves:
            board.push(move)
            print(f"Move made: {move_uci}")
            display_board()
        else:
            print("Invalid move!")
    except ValueError:
        print("Invalid UCI format or move.")


def board_to_tensor(board):
    # Create a tensor to represent the board state (12 layers, 8x8 grid)
    tensor = torch.zeros((12, 8, 8), dtype=torch.float32)  # 12 layers (6 piece types for white and black)

    # Iterate over each square on the chessboard (64 squares in total)
    for square in range(64):
        piece = board.piece_at(square)

        if piece:  # If there is a piece at this square
            row, col = divmod(square, 8)  # Get the row and column from the square number

            # Determine the piece type and color
            piece_type = piece.piece_type
            color = piece.color

            # Determine the index in the tensor: for white pieces, use positive values
            # For black pieces, use negative values and the index to determine layer
            layer = piece_type - 1  # piece_type 1 corresponds to layer 0, etc.

            if color == chess.BLACK:
                layer += 6  # Black pieces are in the second half of the tensor (layers 6 to 11)

            # Add the piece's value to the appropriate position in the tensor
            tensor[layer, row, col] = 1  # We are simply marking the presence of a piece, so use a value of 1

    # Flatten the tensor to match the input shape expected by the neural network
    tensor = tensor.view(-1)  # Flatten the 12x8x8 tensor into a 768-length vector
    return tensor

def print_board_tensor():
    tensor = board_to_tensor(board)
    print("Tensor shape:", tensor.shape)
    print(tensor)

game_number = 1

rla_agent = ChessRLAI(model=DQN())

# Function to handle the Player vs RLAI mode
def play_pvrla():
    # Initialize the ChessRLAI agent for the AI

    while not board.is_game_over():
        display_board()

        print(" ")
        # Print all legal moves
        print("Legal moves: ")
        print(board.legal_moves)

        # Get the move from the player (example: "e2e4")
        move_uci = input("Enter your move (e.g. 'e2e4'): ")
        print_board_tensor()

        # Try to make the player's move
        make_move(move_uci)

        # If it's the RLAI's turn (assume AI plays after player)
        if not board.is_game_over():
            action = rla_agent.find_best_move_with_q_values(board)    # Choose an action based on the state

        # Get the list of legal moves
        legal_moves = list(board.legal_moves)

        # Make sure the action is one of the legal moves
        if action in legal_moves:
            move = action  # Get the corresponding move from legal_moves
            print(f"{'White' if board.turn == chess.WHITE else 'Black'} AI plays: {move}")
            make_move(move.uci())  # Apply the move
            display_board()  # Ensure the board is redrawn after the move
            root.update_idletasks()  # Force UI updates
        else:
            continue

        # Introduce a small delay for better viewing of the moves
        root.after(20)  # Non-blocking way to add a delay for viewing purposes

    # After the game ends, record total reward and loss
    game_numbers.append(game_number)
    total_rewards.append(rla_agent.total_reward)
    losses.append(rla_agent.total_loss)


    # Once the game is over, print the result
    print("Game Over!")
    print(f"Result: {board.result()}")

    result = board.result()
    if result == "1-0":  # White wins
        white_wins += 1
        print("Player wins!")
    elif result == "0-1":  # Black wins
        black_wins += 1
        print("AI wins!")
    else:
        stalemates += 1
        print("It's a draw!")

    print("white wins: ", white_wins)
    print("black wins: ", black_wins)
    print("stalemates: ", stalemates)

    game_number += 1

    board.set_board_fen(chess.STARTING_BOARD_FEN)
    start_game()

white_wins = 0
black_wins = 0
stalemates = 0



# Main function to start the game based on the selected mode
def start_game():
    display_board()
    print("Starting Player vs RLAI mode...")
    play_pvrla()
    start_game()

# Start the game
start_game()

# Initial board display
display_board()