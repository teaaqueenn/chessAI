import chess
import tkinter as tk
from tkinter import messagebox
import threading
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random
import time
import matplotlib.pyplot as plt
from matplotlib import figure
from matplotlib import pyplot
import copy
from tkinter import simpledialog



GREEN = f'#395631'
BEIGE = f'#d1b281'


game_numbers = []
game_numbers.append(0.0)
total_rewards = []
sum_rewards = []
sum_rewards.append(0.0)
turn_rewards = []
turn_numbers =[]
average_game_q_values = []
average_game_q_values.append(0.0)
turn_q_values = []
best_move_q_value = 0.0
average_q_value = 0.0
turn_number = 0.0
white_wins = 0
black_wins = 0
stalemates = 0
highlighted_square = None  # Global variable to track highlighted square
highlighted_legal_moves = []           # List to store legal moves for the selected piece
game_total_rewards = []  # List to store total rewards for each game
game_total_rewards.append(0.0)

fig = None
ax = None

uci_click = ""
firstClick = True  # Boolean to track first click

plt.ion()

# Initialize the chess board
board = chess.Board()

# Colors for the chess pieces
white_pieces = {
    chess.PAWN: "♙", chess.KNIGHT: "♘", chess.BISHOP: "♗", chess.ROOK: "♖",
    chess.QUEEN: "♕", chess.KING: "♔"
}
black_pieces = {
    chess.PAWN: "♟", chess.KNIGHT: "♞", chess.BISHOP: "♝", chess.ROOK: "♜",
    chess.QUEEN: "♛", chess.KING: "♚"
}

# Tkinter setup
root = tk.Tk()
root.title("Chess Game")

# Create a canvas to draw the chessboard
canvas = tk.Canvas(root, width=670, height=670)
canvas.pack()

# Define the Deep Q-Network (DQN) model
import torch
import torch.nn as nn



class DQN(nn.Module):
    def __init__(self):
        super(DQN, self).__init__()
        
        # Input layer: 12 * 8 * 8 = 768 (flattened input)
        self.fc1 = nn.Linear(12 * 8 * 8, 2048)  # First hidden layer with 2048 units
        self.fc2 = nn.Linear(2048, 2048)        # Second hidden layer with 2048 units
        self.fc3 = nn.Linear(2048, 1024)        # Third hidden layer with 1024 units
        self.fc4 = nn.Linear(1024, 1024)        # Fourth hidden layer with 1024 units
        self.fc5 = nn.Linear(1024, 512)         # Fifth hidden layer with 512 units
        self.fc6 = nn.Linear(512, 512)          # Sixth hidden layer with 512 units
        self.fc7 = nn.Linear(512, 256)          # Seventh hidden layer with 256 units
        self.fc8 = nn.Linear(256, 256)          # Eighth hidden layer with 256 units
        self.fc9 = nn.Linear(256, 128)          # Ninth hidden layer with 128 units
        self.fc10 = nn.Linear(128, 128)         # Tenth hidden layer with 128 units
        self.fc11 = nn.Linear(128, 64)          # Eleventh hidden layer with 64 units
        self.fc12 = nn.Linear(64, 64)           # Twelfth hidden layer with 64 units
        self.fc13 = nn.Linear(64, 32)           # Thirteenth hidden layer with 32 units
        self.fc14 = nn.Linear(32, 32)           # Fourteenth hidden layer with 32 units
        self.fc15 = nn.Linear(32, 16)           # Fifteenth hidden layer with 16 units
        self.fc16 = nn.Linear(16, 16)           # Sixteenth hidden layer with 16 units
        self.fc17 = nn.Linear(16, 8)            # Seventeenth hidden layer with 8 units
        self.fc18 = nn.Linear(8, 8)             # Eighteenth hidden layer with 8 units
        self.fc19 = nn.Linear(8, 4)             # Nineteenth hidden layer with 4 units
        self.fc20 = nn.Linear(4, 4672)  # Output layer with 4672 units (for possible moves)

        # Dropout layer to prevent overfitting (applied after each hidden layer)
        self.dropout = nn.Dropout(0.3)  # Dropout with 30% probability of zeroing out inputs

    def forward(self, x):
        # Passing the input through all the layers with ReLU activation
        x = torch.relu(self.fc1(x))
        x = self.dropout(x)
        
        x = torch.relu(self.fc2(x))
        x = self.dropout(x)

        x = torch.relu(self.fc3(x))
        x = self.dropout(x)

        x = torch.relu(self.fc4(x))
        x = self.dropout(x)

        x = torch.relu(self.fc5(x))
        x = self.dropout(x)

        x = torch.relu(self.fc6(x))
        x = self.dropout(x)

        x = torch.relu(self.fc7(x))
        x = self.dropout(x)

        x = torch.relu(self.fc8(x))
        x = self.dropout(x)

        x = torch.relu(self.fc9(x))
        x = self.dropout(x)

        x = torch.relu(self.fc10(x))
        x = self.dropout(x)

        x = torch.relu(self.fc11(x))
        x = self.dropout(x)

        x = torch.relu(self.fc12(x))
        x = self.dropout(x)

        x = torch.relu(self.fc13(x))
        x = self.dropout(x)

        x = torch.relu(self.fc14(x))
        x = self.dropout(x)

        x = torch.relu(self.fc15(x))
        x = self.dropout(x)

        x = torch.relu(self.fc16(x))
        x = self.dropout(x)

        x = torch.relu(self.fc17(x))
        x = self.dropout(x)

        x = torch.relu(self.fc18(x))
        x = self.dropout(x)

        x = self.fc19(x)  # No activation for the last hidden layer
        x = self.fc20(x)  # Output layer with dynamic action space size
        return x


class ChessRLAI:
    def __init__(self, model = DQN(), epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995, learning_rate=0.001, gamma=0.99):
        self.model = model
        self.epsilon = epsilon  # Initial epsilon
        self.epsilon_min = epsilon_min  # Minimum value of epsilon
        self.epsilon_decay = epsilon_decay  # Decay factor
        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
        self.loss_fn = nn.MSELoss()  # Loss function for Q-values
        self.gamma = gamma  # Discount factor for future rewards

        self.total_reward = 0.0
        self.sum_reward = 0.0
        self.total_loss = 0.0
        self.turn_reward = 0.0

    def is_fork(self, board, square):
        """
        Check if the piece at `square` is performing a fork.
        A fork happens when one piece attacks two or more pieces of the opponent.
        """
        piece = board.piece_at(square)
        if piece is None or piece.color != chess.BLACK:
            return False

        # Get all attacked squares
        attacked_squares = board.attacks(square)
        
        # Count how many opponent pieces are being attacked
        attacked_pieces = [board.piece_at(sq) for sq in attacked_squares if board.piece_at(sq) and board.piece_at(sq).color == chess.WHITE]
        
        # If two or more opponent pieces are attacked, it's a fork
        return len(attacked_pieces) >= 2

    def is_double_attack(self, board, square):
        """
        Check if the piece at `square` is attacking two opponent pieces at the same time.
        A double attack occurs when one piece attacks two opponent pieces simultaneously.
        """
        piece = board.piece_at(square)
        if piece is None or piece.color != chess.BLACK:
            return False

        # Get all attacked squares
        attacked_squares = board.attacks(square)
        
        # Count how many opponent pieces are being attacked
        attacked_pieces = [board.piece_at(sq) for sq in attacked_squares if board.piece_at(sq) and board.piece_at(sq).color == chess.WHITE]
        
        # If two or more opponent pieces are attacked simultaneously, it's a double attack
        return len(attacked_pieces) >= 2
    
    def is_back_rank_threat(self, board):
        """
        Check if there is a back rank threat against the opponent's king.
        A back rank threat occurs when the opponent's king is on the back rank
        and there is a potential check from a black piece (typically a rook or queen).
        """
        # Get the location of the white king (since we're checking for black's threat)
        white_king_square = board.king(chess.WHITE)
        
        # Check if the white king is on the 1st rank (back rank for white)
        if white_king_square not in chess.SQUARES[0:8]:  # First rank is 0-7 (row 1)
            return False  # White king is not on the back rank
        
        # Check if the opponent's back rank is blocked or if there are escape squares
        for square in chess.SQUARES[0:8]:
            piece = board.piece_at(square)
            if piece and piece.color == chess.WHITE:
                # If there's a white piece on the back rank, the back rank is "blocked"
                # and may provide a potential check from a black piece.
                return True
        
        # If there's a black piece threatening the back rank (typically rooks or queens)
        for square in chess.SQUARES[0:8]:
            # Check if a black piece is on the same rank as the white king
            piece = board.piece_at(square)
            if piece and piece.color == chess.BLACK:
                if piece.piece_type in [chess.ROOK, chess.QUEEN]:  # Corrected line
                    # If a rook or queen is attacking on the back rank, it's a threat
                    if board.is_check():
                        return True
        
        return False
    
    def is_piece_vulnerable(self, board, square):
        # Determine which color is playing
        color = board.turn
        
        # Iterate over all opponent's pieces
        opponent_color = not color
        for square in chess.SQUARES:
            piece = board.piece_at(square)
            if piece and piece.color == opponent_color:
                # Check if the opponent can move to the target square
                legal_moves = board.legal_moves
                for move in legal_moves:
                    if move.to_square == square:
                        return True
        return False
    
    def is_isolated(self, board, square):
        # Check if a pawn on the square is isolated
        file = chess.square_file(square)
        # Check if there are pawns on the adjacent files
        left = chess.square(file-1, chess.square_rank(square)) if file > 0 else None
        right = chess.square(file+1, chess.square_rank(square)) if file < 7 else None
        
        left_pawn = board.piece_at(left) if left is not None else None
        right_pawn = board.piece_at(right) if right is not None else None
        
        return (left_pawn is None or left_pawn.piece_type != chess.PAWN) and (right_pawn is None or right_pawn.piece_type != chess.PAWN)
    
    def is_passed_pawn(board, square, color):
        # A passed pawn has no opponent pawns on the same or adjacent files
        file = chess.square_file(square)
        rank = chess.square_rank(square)
        opponent_color = not color
        
        for rank_offset in range(-1, 2):
            for file_offset in range(-1, 2):
                target_square = chess.square(file + file_offset, rank + rank_offset)
                if board.is_on_board(target_square):
                    piece = board.piece_at(target_square)
                    if piece and piece.color == opponent_color and piece.piece_type == chess.PAWN:
                        return False  # There is an opposing pawn that can block
        return True  # No opposing pawns can block

    def is_hanging(board, square, color):
        piece = board.piece_at(square)
        if not piece:
            return False
        
        # Check if the piece is under attack by an opponent piece
        for target_square in chess.SQUARES:
            if board.is_attacked_by(not color, target_square):
                if board.piece_at(target_square).color != color:
                    return True  # The piece is under attack
        return False




    def reward_of_move(self, move, board) -> float:
        """
        Calculate the reward for a given move, considering both tactical motifs and material factors.
        """
        reward = 0.0

        # Define piece values for each type of piece (based on standard chess piece values)
        piece_value = {
            chess.PAWN: 1.0,
            chess.KNIGHT: 3.0,
            chess.BISHOP: 3.0,
            chess.ROOK: 5.0,  # Adjusted value for rooks (standard: 5, but more dynamic based on board context)
            chess.QUEEN: 9.0,
            chess.KING: 0.0  # King does not have a direct value in terms of material
        }

        # Importance scale for different chess concepts (scaled from 1-10)
        importance_scale = {
            "material_gain_loss": 9.5,
            "check_checkmate": 9.45,
            "king_safety": 9.4,
            "piece_development": 8.95,
            "control_of_center": 8.9,
            "threats_and_tactics": 8.85,
            "pawn_structure": 8.75,
            "piece_activity": 8.6,
            "attacks_on_king": 8.55,
            "rook_queen_activity": 8.5,
            "pawn_breaks": 8.45,
            "passed_pawns": 8.2,
            "creating_plan": 8.15,
            "knight_outposts": 8.1,
            "piece_protection": 8.05,
            "avoiding_hanging_pieces": 8.0,
            "promotion_threats": 7.95,
            "opposite_color_bishops": 7.75,
            "back_rank_weaknesses": 7.6,
            "isolated_pawns": 7.55,
            "doubled_pawns": 7.5,
            "connected_pawns": 7.4,
            "backward_pawns": 7.3,
            "weak_squares": 7.2,
            "en_passant": 6.0
        }

        # Create a copy of the board to simulate the move
        board_copy = board.copy()

        # Step 1: Material gain or loss (scaled reward based on piece value)
        if board_copy.is_capture(move):
            captured_piece = board_copy.piece_at(move.to_square)
            if captured_piece:
                piece_type = captured_piece.piece_type
                reward += importance_scale["material_gain_loss"] * piece_value.get(piece_type, 0.0)

        # Step 2: Check or checkmate (scaled reward)
        board_copy.push(move)
        if board_copy.is_checkmate():
            reward += importance_scale["check_checkmate"]
        elif board_copy.is_check():
            reward += importance_scale["check_checkmate"] * 0.5  # Slightly smaller reward for check
        board_copy.pop()

        # Step 3: King safety (penalize if king becomes exposed)
        if board_copy.is_check():
            reward -= importance_scale["king_safety"] * 0.5  # Penalize for putting the king in check

        # Step 4: Piece development (reward if piece is developed)
        move_piece = board_copy.piece_at(move.from_square)
        if move_piece:
            piece_type = move_piece.piece_type
            if piece_type == chess.PAWN:
                if move.to_square in [chess.C3, chess.F3, chess.C6, chess.F6]:  # Example: central pawn pushes
                    reward += importance_scale["piece_development"]
            elif piece_type in [chess.KNIGHT, chess.BISHOP]:
                if move.to_square in [chess.D3, chess.E3, chess.D6, chess.E6]:  # Example: key squares for piece development
                    reward += importance_scale["piece_development"]

        # Step 5: Control of the center (reward for controlling central squares)
        if move.to_square in [chess.D4, chess.E4, chess.D5, chess.E5]:  # Example: controlling the center
            reward += importance_scale["control_of_center"]

        # Step 6: Threats and tactics (reward for tactical moves like forks, pins) with multipliers
        if self.is_fork(board_copy, move.to_square):
            reward += importance_scale["threats_and_tactics"] * 1.5  
        if board.is_pinned(chess.BLACK, move.to_square):
            reward += importance_scale["threats_and_tactics"] * 1.0
        if self.is_double_attack(board_copy, move.to_square):
            reward += importance_scale["threats_and_tactics"] * 1.2
        if self.is_back_rank_threat(board_copy):
            reward += importance_scale["threats_and_tactics"] * 0.5

        if move_piece == chess.QUEEN:
            if board_copy.is_check():  # Threatening the opponent's king with the queen
                reward += importance_scale["threats_and_tactics"]

        # Step 7: Pawn structure (penalize if structure is weakened)
        if move_piece == chess.PAWN:
            if self.is_isolated(board_copy,move.to_square):
                reward -= importance_scale["pawn_structure"] * 0.5  # Penalize isolated pawns

        # Step 8: Piece activity (reward if piece becomes more active)
        if move_piece == chess.ROOK:
            if move.to_square in [chess.D1, chess.E1, chess.D8, chess.E8]:  # Example: activating rooks
                reward += importance_scale["rook_queen_activity"]

        # Step 9: Attacks on the opponent's king (reward for attacking moves)
        if board_copy.is_checkmate():
            reward += importance_scale["attacks_on_king"]

        # Step 10: Passed pawns (reward if the move advances a passed pawn)
        if move_piece == chess.PAWN:
            if self.is_passed_pawn(board_copy, move.to_square):
                reward += importance_scale["passed_pawns"]

        # Step 11: Creating a plan (reward for structured, purposeful moves)
        if move_piece == chess.KNIGHT:
            reward += importance_scale["creating_plan"]

        # Step 12: Knight outposts (reward for placing knights on outposts)
        if move_piece == chess.KNIGHT:
            if move.to_square in [chess.D4, chess.E5, chess.D5, chess.E4]:
                reward += importance_scale["knight_outposts"]

        # Step 13: Piece protection (reward for moves that protect valuable pieces)
        if move_piece in [chess.KNIGHT, chess.ROOK, chess.BISHOP]:
            reward += importance_scale["piece_protection"]

        # Step 14: Avoiding hanging pieces (penalize if a piece is left unprotected)
        if move_piece in [chess.KNIGHT, chess.ROOK, chess.BISHOP, chess.QUEEN]:
            if self.is_hanging(board_copy, move.to_square):
                reward -= importance_scale["avoiding_hanging_pieces"]

        # Step 15: Promotion threats (reward for advancing a pawn that threatens promotion)
        if move_piece == chess.PAWN:
            if move.to_square in [chess.A8, chess.H8]:
                reward += importance_scale["promotion_threats"]

        # Step 16: Opposite-color bishops (reward if there is a strategic imbalance created)
        if move_piece == chess.BISHOP:
            if board_copy.is_opposite_color_bishops():
                reward += importance_scale["opposite_color_bishops"]

        # Step 17: Back-rank weaknesses (penalize for leaving a back-rank weakness)
        if move_piece == chess.PAWN:
            if board_copy.is_back_rank_weak():
                reward -= importance_scale["back_rank_weaknesses"]

        # Step 18: Isolated pawns (penalize isolated pawns)
        if move_piece == chess.PAWN:
            if board_copy.is_isolated_pawn(move.to_square):
                reward -= importance_scale["isolated_pawns"]

        # Step 19: Doubled pawns (penalize for doubled pawns)
        if move_piece == chess.PAWN:
            if board_copy.is_doubled_pawn(move.to_square):
                reward -= importance_scale["doubled_pawns"]

        # Step 20: Connected pawns (reward for connected pawns)
        if move_piece == chess.PAWN:
            if board_copy.is_connected_pawn(move.to_square):
                reward += importance_scale["connected_pawns"]

        # Step 21: Backward pawns (penalize if move creates a backward pawn)
        if move_piece == chess.PAWN:
            if board_copy.is_backward_pawn(move.to_square):
                reward -= importance_scale["backward_pawns"]

        # Step 22: Weak squares (penalize if a weak square is created)
        if move_piece in [chess.KNIGHT, chess.BISHOP, chess.ROOK]:
            if board_copy.is_weak_square(move.to_square):
                reward -= importance_scale["weak_squares"]

        # Step 23: En passant capture (reward for en passant)
        if board_copy.is_en_passant(move):
            reward += importance_scale["en_passant"]

        # Step 24: Attacking opponent’s weak piece (reward for attacking vulnerable pieces)
        if move_piece in [chess.QUEEN, chess.ROOK, chess.KNIGHT]:
            if board_copy.is_attacking_weak_piece(move.to_square):
                reward += importance_scale["material_gain_loss"] * 0.75

        # Step 25: Counter-attacks (reward for moves that create counter threats)
        if board_copy.is_check():
            reward += importance_scale["threats_and_tactics"] * 0.5  # Reward for counter-threats

        
        if self.is_piece_vulnerable(board_copy, move.to_square):
            captured_piece = board_copy.piece_at(move.to_square)
            if captured_piece:
                piece_type = captured_piece.piece_type
                reward -= importance_scale["material_gain_loss"] * piece_value.get(piece_type, 0.0) * 0.2

        reward -= 0.05

        return reward


    def find_best_move_with_q_values(self, board, gamma=0.99, alpha=0.1):
        """
        Calculate the Q-values for each legal move, update the Q-value using Q-learning,
        and return the move with the highest Q-value.

        Parameters:
            board (chess.Board): The current state of the board.
            legal_moves (list): A list of legal moves (chess.Move objects).
            reward (float): The reward for making the move.
            gamma (float): The discount factor for future rewards.
            alpha (float): The learning rate for updating Q-values.

        Returns:
            chess.Move: The move with the highest Q-value (in UCI notation).
        """
        # Initialize a list to store the Q-values for each legal move
        q_values = []

        legal_moves = list(board.legal_moves)

        rand_val = random.random()

        print("rand val: ", rand_val)
        print("epsilon val: ", self.epsilon)

        # Initialize best_move outside the if-else block
        best_move = None
        best_move_q_value = 0.0
        if rand_val < self.epsilon:
            # Explore: Choose a random legal move
            best_move = random.choice(legal_moves)
            print("using random vals")
            # Calculate Q-value for the random move, even though it's not used in decision making
            state_tensor = board_to_tensor(board).unsqueeze(0)  # Convert the current board state to tensor
            current_q_values = self.model(state_tensor).squeeze(0)  # Get the Q-values for the current state

            # Find the index of the random move in the legal_moves list
            movedex = legal_moves.index(best_move)

            # Get the Q-value for the random move
            random_move_q_value = current_q_values[movedex].item()
            print(f"Q-value of random move {best_move}: {random_move_q_value}")

            # Since it's a random move, we can assign a default Q-value (e.g., 0) for it
            best_move_q_value = random_move_q_value

            self.total_reward += self.reward_of_move(best_move, board)
        else:
            print("using q-vals")

            # Exploit: Choose the move with the highest Q-value from the model's output
            with torch.no_grad():
                # Loop through each legal move and calculate the Q-value
                for move in legal_moves:
                    # Step 1: Convert the current board state to tensor
                    state_tensor = board_to_tensor(board).unsqueeze(0)  # Add batch dimension

                    # Step 2: Get the Q-values for the current state
                    current_q_values = self.model(state_tensor).squeeze(0)

                    # Step 3: Simulate the move by copying the board and making the move on the clone
                    cloned_board = copy.deepcopy(board)  # Clone the board
                    cloned_board.push(move)  # Make the move on the cloned board

                    # Step 4: Convert the new state after the move to tensor
                    next_state_tensor = board_to_tensor(cloned_board).unsqueeze(0)  # Get the next state tensor after the move

                    # Step 5: Get the Q-values for the next state
                    next_q_values = self.model(next_state_tensor).squeeze(0)

                    # Step 6: Calculate the maximum Q-value for the next state
                    max_next_q_value = torch.max(next_q_values).item()

                    # Step 7: Find the index of the move in the model's Q-value predictions
                    move_index = legal_moves.index(move)

                    # Step 8: Get the current Q-value for the move
                    current_q_value = current_q_values[move_index].item()

                    # Step 9: Calculate the new Q-value using the Q-learning equation
                    updated_q_value = current_q_value + alpha * (self.reward_of_move(move, board) + gamma * max_next_q_value - current_q_value)

                    # Append the updated Q-value to the list
                    q_values.append(updated_q_value)

                # Convert q_values list to tensor
                q_values_tensor = torch.tensor(q_values)

                # Step 10: Calculate the loss (using MSE) and update Q-value
                # We should update the Q-value for the move that was selected
                updated_q_value_tensor = torch.tensor([updated_q_value])  # Convert updated_q_value to tensor

                # Ensure q_values_tensor is a tensor of shape (N,)
                loss = self.loss_fn(q_values_tensor, updated_q_value_tensor)

                # Update the total loss
                self.total_loss += loss.item()

                # Step 11: Find the move with the highest Q-value (from q_values)
                best_move_index = np.argmax(q_values)
                best_move = legal_moves[best_move_index]

                best_move_q_value = q_values[best_move_index]

                self.turn_reward = self.reward_of_move(best_move, board)

                self.total_reward += self.reward_of_move(best_move, board)

                self.sum_reward += self.reward_of_move(best_move, board)
                
        turn_q_values.append(best_move_q_value)
        

        # Decay epsilon after each move
        self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)
        
        return best_move
    


def plot_results():
    """
    Creates or updates a 1x3 grid of subplots:
    - First subplot: Total Reward vs Game Number with losses
    - Second subplot: Pie chart for black wins, white wins, and stalemates (only if count > 0)
    - Third subplot: Total Reward vs Turns
    """
    global white_wins, black_wins, stalemates, game_numbers, total_rewards, fig, ax, turn_q_values, average_game_q_values
    
    # Create a 1x3 grid of subplots if not created yet
    if fig is None or ax is None:
        fig, ax = plt.subplots(2, 3, figsize=(10, 8)) 

    #print("game numbers array: ", game_numbers)
    print("turn numbers array: ", turn_numbers)
    #print("total rewards array: ", total_rewards)
    #print("game total rewards array: ", game_total_rewards)
    print("turn rewards array: ", turn_rewards)
    
    # First subplot: Total Reward vs Turns
    ax[0, 0].cla()  # Clear the first subplot
    ax[0, 0].plot(turn_numbers, total_rewards, marker='o', color='b', label="Total Reward")
    ax[0, 0].set_title("Total Reward vs Turns")
    ax[0, 0].set_xlabel("Turns")
    ax[0, 0].set_ylabel("Total Reward")
    ax[0, 0].grid(True)
    ax[0, 0].legend()

    # Fourth subplot: Turn Reward vs Turn Number
    ax[0, 1].cla()  # Clear the fourth subplot
    ax[0, 1].plot(turn_numbers, turn_rewards, marker='x', color='r', label="Turn Reward")
    ax[0, 1].set_title("Turn Reward vs Turns")
    ax[0, 1].set_xlabel("Turns")
    ax[0, 1].set_ylabel("Turn Reward")
    ax[0, 1].grid(True)
    ax[0, 1].legend()

    ax[0,2].cla()  # Clear the fourth subplot
    ax[0,2].plot(game_numbers, average_game_q_values, marker='x', color='r', label="Game Q-Value")
    ax[0,2].set_title("Average Game Q-Value vs Game Number")
    ax[0,2].set_xlabel("Game Number")
    ax[0,2].set_ylabel("Average Game Q-Value")
    ax[0,2].grid(True)
    ax[0,2].legend()

    # Second subplot: Pie chart for black wins, white wins, and stalemates (only if count > 0)
    total_games = white_wins + black_wins + stalemates
    ax[1, 0].cla()  # Clear the second subplot
    if total_games > 0:
        labels = ['White Wins', 'Black Wins', 'Stalemates']
        colors = ['#FF9999', '#66B2FF', '#99FF99']  # Different colors for each slice
        sizes = [white_wins, black_wins, stalemates]
        ax[1, 0].pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90, colors=colors)
        ax[1, 0].axis('equal')  # Equal aspect ratio ensures the pie chart is circular
        ax[1, 0].set_title("Game Outcome Distribution")

    # Third subplot: Total Reward vs Game Number (without losses scatter)
    ax[1, 1].cla()  # Clear the third subplot
    
    # Third subplot: plot total rewards vs game numbers
    ax[1, 1].plot(game_numbers, game_total_rewards, marker='o', color='g', label="Total Reward Difference per Game")
    ax[1, 1].set_title("Total Reward vs Game Number")
    ax[1, 1].set_xlabel("Game Number")
    ax[1, 1].set_ylabel("Total Reward")
    ax[1, 1].grid(True)
    ax[1, 1].legend()

    ax[1,2].cla()  # Clear the fourth subplot
    ax[1,2].plot(turn_numbers, turn_q_values, marker='x', color='r', label="Turn Q-Value")
    ax[1,2].set_title("Turn Q-Value vs Turns")
    ax[1,2].set_xlabel("Turns")
    ax[1,2].set_ylabel("Turn Q-Value")
    ax[1,2].grid(True)
    ax[1,2].legend()

    plt.tight_layout()

    # Update the plot
    plt.draw()
    plt.pause(0.1)  # Pause to allow the plot to update



# Function to handle click events
def on_click(event):
    global highlighted_square, highlighted_legal_moves, firstClick, uci_click, turn  # Access global variables

    # Calculate the row and column of the clicked square based on the canvas click
    col = event.x // 80
    row = event.y // 80

    # Convert to chess notation (e.g., "a1", "h8") based on the reversed orientation
    chess_col = chr(col + 97)  # Convert column to letter ('a' to 'h')
    chess_row = 8 - row        # Convert row to number ('1' to '8') based on flipped orientation
    clicked_square = chess_col + str(chess_row)   
    # Convert the file (char) to an index (0-7)
    file_index = ord(chess_col) - 97  # 'a' -> 0, 'b' -> 1, ..., 'h' -> 7
    
    # Convert the rank (char) to an integer (1 -> 0, 2 -> 1, ..., 8 -> 7)
    rank_index = int(chess_row) - 1  # "1" -> 0, "2" -> 1, ..., "8" -> 7
                
    # Use chess.square to convert (file_index, rank_index) to the square index
    square = chess.square(file_index, rank_index)
    

    # Print the chessboard notation (e.g., "a1", "h8") for debugging
    print(f"Clicked on square: {clicked_square}")
    print(f"Square index: {square}")

    # Check if there's a piece on the clicked square
    piece = board.piece_at(chess.square(col, 7 - row))

    # If it's the first click and the user selects a piece
    if firstClick:
        if piece:
            # Get legal moves for the clicked square (if it's a piece)
            highlighted_legal_moves = get_legal_moves(board, clicked_square)
            highlighted_square = (row, col)  # Highlight the selected piece's position

            # Debugging: Check if the highlighted_legal_moves is being populated
            print(f"Highlighted legal moves: {highlighted_legal_moves}")
            uci_click = clicked_square
            print("uci clicked = ", uci_click)
            # After the first click, switch to second click mode
            firstClick = False
        else:
            print("No piece at clicked square. Please select a piece.")
    
    else:
        # If it's the second click, check if the square is a legal move
        print("legal moves array: ", highlighted_legal_moves)
        if square in highlighted_legal_moves:
            print("Legal move found!")
            uci_click += clicked_square
            print("uci clicked = ", uci_click)
            makePlayerMove(uci_click)  # Execute the move 
            firstClick = True   
            uci_click = ""
            highlighted_square = None
            highlighted_legal_moves = []
            turn.set(1)
            # Reset to first click mode after move
        elif piece:
            print("Clicked on a piece: " + str(piece) + ". At index: " + str(square) + ". Clicked on:" + clicked_square)
            highlighted_square = None
            highlighted_legal_moves = []
            firstClick = True  # Go back to selecting a new piece
        else:
            print("Clicked on an invalid square or no piece to move. Clicked on:" + str(square))
            # Optionally, reset the highlighted square and legal moves
            highlighted_square = None
            highlighted_legal_moves = []
            firstClick = True  # Go back to selecting a new piece


    # Redraw the board with the updated highlighted square and legal moves
    canvas.delete("all")  # Clear the canvas
    display_board()        # Redraw the board with updated highlights


# Function to get legal moves for a given square (in UCI notation)
def get_legal_moves(board, clicked_square):
    legal_moves = []
    moves = board.legal_moves  # This should be a set of LegalMove objects
    
    for move in moves:
        # Convert the move to UCI notation (e.g., "e2e4")
        uci_move = move.uci()
        
        # Check if the first two characters of UCI move match the clicked square
        if uci_move[:2] == clicked_square:
            # Extract the destination square in UCI format (e.g., "e4")
            destination_square_uci = uci_move[2:4]
            file_char = destination_square_uci[0]  # e.g., "e"
            rank_char = destination_square_uci[1]  # e.g., "4"
            
                        # Convert the file (char) to an index (0-7)
            file_index = ord(file_char) - 97  # 'a' -> 0, 'b' -> 1, ..., 'h' -> 7
            
            # Convert the rank (char) to an integer (1 -> 0, 2 -> 1, ..., 8 -> 7)
            rank_index = int(rank_char) - 1  # "1" -> 0, "2" -> 1, ..., "8" -> 7
                        
            # Use chess.square to convert (file_index, rank_index) to the square index
            destination_square = chess.square(file_index, rank_index)
            
            legal_moves.append(destination_square)
    
    return legal_moves


# Function to display the board in Tkinter
def display_board():
    global highlighted_legal_moves
    square_size = 80
    for row in range(8):
        for col in range(8):
            # Determine the color of the square
            color = GREEN if (row + col) % 2 == 0 else BEIGE
            canvas.create_rectangle(col * square_size, row * square_size,
                                    (col + 1) * square_size, (row + 1) * square_size,
                                    fill=color)

            # Highlight the selected square if it matches
            if highlighted_square == (row, col):
                canvas.create_rectangle(col * square_size, row * square_size,
                                        (col + 1) * square_size, (row + 1) * square_size,
                                        outline="red", width=3)  # Red border for selected square

            # Highlight the legal moves (blue border)
            if chess.square(col, 7- row) in highlighted_legal_moves:
                print(f"Legal move found: {chess.square(col, 7 - row)}")
                canvas.create_rectangle(col * square_size, row * square_size,
                                        (col + 1) * square_size, (row + 1) * square_size,
                                        outline="blue", width=2)  # Blue border for legal moves

            # Get the piece at the current square
            piece = board.piece_at(chess.square(col, 7 - row))  # Flip y-axis for correct orientation
            if piece:
                piece_char = ""
                if piece.color == chess.WHITE:
                    piece_char = white_pieces.get(piece.piece_type, "")
                else:
                    piece_char = black_pieces.get(piece.piece_type, "")
                canvas.create_text(col * square_size + square_size / 2,
                                   row * square_size + square_size / 2,
                                   text=piece_char, font=("Times New Roman", 36))

            # Display the square number (0 to 63) in the center of the square
            square_number = chess.square(col, 7 - row)  # Get the square number from (col, row)
            canvas.create_text(col * square_size + square_size / 2,
                               row * square_size + square_size / 2,
                               text=str(square_number), font=("Times New Roman", 12, "bold"), fill="black")

    # Draw row labels (1-8) on the left side of the board
    for row in range(8):
        canvas.create_text(8 * square_size + 10, row * square_size + square_size / 2,  # Right side positioning
                           text=str(8 - row), font=("Times New Roman", 14))

    # Draw column labels (a-h) at the bottom of the board
    for col in range(8):
        canvas.create_text(col * square_size + square_size / 2,
                           8 * square_size + 10,  # Position below the board
                           text=chr(ord('a') + col), font=("Times New Roman", 14))
        


def makePlayerMove(move_uci):
    global board
    try:
        # Apply the move using UCI notation
        move = chess.Move.from_uci(move_uci)
        piece = board.piece_at(move.from_square)
        # Check for promotion
        if piece and piece.piece_type == chess.PAWN and \
            ((move.to_square < 65 and move.to_square > 55) or (move.to_square < 8 and move.to_square > -1)):  
            # Pawn moved to promotion rank (1st or 8th rank)
            print("promoting")
            
            # Ask for promotion choice using Tkinter dialog
            root = tk.Tk()
            root.withdraw()  # Hide the main window
            promotion_choice = simpledialog.askstring("Pawn Promotion", 
                                                      "Promote to (Q [Queen], R [Rook], B [Bishop], N [Knight]):", 
                                                      parent=root)
            # Map promotion choice to the correct piece type
            piece_map = {'Q': chess.QUEEN, 'R': chess.ROOK, 'B': chess.BISHOP, 'N': chess.KNIGHT}
            
            if promotion_choice in piece_map:  # Validate input
                promotion_piece = piece_map[promotion_choice]  # Get the piece type constant
                print("promotion piece: ", promotion_piece)
                
                # Create the promoted move
                move = chess.Move(move.from_square, move.to_square, promotion=promotion_piece)
                print("new promotion move: ", move)
                print(f"Pawn promoting to {promotion_choice}")
        if move in board.legal_moves:
            board.push(move)
            print(f"Move made: {move_uci}")
            display_board()
        else:
            print("Invalid move!")
    except ValueError:
        print("Invalid UCI format or move.")

# Function to handle a player's move from the console
def make_move(move_uci):
    global board
    try:
        # Apply the move using UCI notation
        move = chess.Move.from_uci(move_uci)
        if move in board.legal_moves:
            board.push(move)
            print(f"Move made: {move_uci}")
            display_board()
        else:
            print("Invalid move!")
    except ValueError:
        print("Invalid UCI format or move.")


def board_to_tensor(board):
    # Create a tensor to represent the board state (12 layers, 8x8 grid)
    tensor = torch.zeros((12, 8, 8), dtype=torch.float32)  # 12 layers (6 piece types for white and black)

    # Iterate over each square on the chessboard (64 squares in total)
    for square in range(64):
        piece = board.piece_at(square)

        if piece:  # If there is a piece at this square
            row, col = divmod(square, 8)  # Get the row and column from the square number

            # Determine the piece type and color
            piece_type = piece.piece_type
            color = piece.color

            # Determine the index in the tensor: for white pieces, use positive values
            # For black pieces, use negative values and the index to determine layer
            layer = piece_type - 1  # piece_type 1 corresponds to layer 0, etc.

            if color == chess.BLACK:
                layer += 6  # Black pieces are in the second half of the tensor (layers 6 to 11)

            # Add the piece's value to the appropriate position in the tensor
            tensor[layer, row, col] = 1  # We are simply marking the presence of a piece, so use a value of 1

    # Flatten the tensor to match the input shape expected by the neural network
    tensor = tensor.view(-1)  # Flatten the 12x8x8 tensor into a 768-length vector
    return tensor

def print_board_tensor():
    tensor = board_to_tensor(board)
    print("Tensor shape:", tensor.shape)
    print(tensor)

rla_agent = ChessRLAI(model=DQN())
turn = tk.IntVar(value=0)

# Function to handle the Player vs RLAI mode
def play_pvrla():
    global stalemates, white_wins, black_wins, game_numbers, total_rewards, turn_numbers, turn_number, best_move_q_value, average_game_q_values, turn_q_values, average_q_value
    print("play_pvrla started")
    while not board.is_game_over():
        print("turn before display_board: ", turn.get())
        display_board()
        root.update_idletasks()

        print(" ")
        print("Legal moves: ")
        print(board.legal_moves)

        if turn.get() == 0:  # White Player's Turn
            print("White Player's Turn = ", turn.get())
            canvas.bind("<Button-1>", on_click)  # Bind the event to on_click
            root.wait_variable(turn)  # Wait for the player to make a move
            canvas.unbind("<Button-1>")  # Unbind the event after the move is made
            print("Turn after White's move: ", turn.get())
            if turn.get() == 0:
                print("Turn didn't change after White's move, check on_click.")
            turn.set(1)  # Change to Black's turn after player's move
            print("Turn after setting to Black: ", turn.get()) 
            root.update_idletasks()  # Force UI updates
            root.after(100)  # Add a small delay to ensure the turn is updated
            plot_results()
            print("turn after setting to Black: ", turn.get())

        elif turn.get() == 1:  # Black AI's Turn
            print("Black AI's Turn = ", not turn.get())
            action = rla_agent.find_best_move_with_q_values(board)  # Choose an action based on the state
            print("AI action: ", action)
            legal_moves = list(board.legal_moves)

            # Make sure the action is one of the legal moves
            if action in legal_moves:
                move = action  # Get the corresponding move from legal_moves
                print(f"Black AI plays: {move}")
                make_move(move.uci())  # Apply the move
                display_board()  # Ensure the board is redrawn after the move
                root.update_idletasks()  # Force UI updates
                root.after(20)  # Non-blocking way to add a delay for viewing purposes
                turn_number += 1
                turn_numbers.append(turn_number)
                if len(turn_rewards) == 0:
                    turn_reward = rla_agent.total_reward
                else:
                    turn_reward = ((rla_agent.total_reward)-(total_rewards[-1]))
                print("turn reward: ", turn_reward)
                turn_rewards.append(turn_reward)
                total_rewards.append(rla_agent.total_reward)
                average_q_value += best_move_q_value
                average_q_value /= 2
                plot_results()
            else:
                continue

            turn.set(0)  # Change to White's turn after AI's move

        # Introduce a small delay for better viewing of the moves
        root.after(20)  # Non-blocking way to add a delay for viewing purposes

    game_numbers.append(game_numbers[-1] + 1)
    average_game_q_values.append(average_q_value)
    average_q_value = 0

    # Once the game is over, print the result
    print("Game Over!")
    print(f"Result: {board.result()}")


    current_game_total_reward = rla_agent.sum_reward  # Get the total reward for the current game

    if len(game_total_rewards) == 0:
        # For the first game, just append the total reward
        game_total_rewards.append(current_game_total_reward)
    else:
        # For subsequent games, append the difference from the last game
        last_game_total_reward = game_total_rewards[-1]  # Get the last appended total reward
        reward_difference = current_game_total_reward - last_game_total_reward  # Calculate the difference
        game_total_rewards.append(reward_difference)  # Store the difference in total rewards

    result = board.result()
    if result == "1-0":  # White wins
        white_wins += 1
        rla_agent.total_reward -= 100
        print("Player wins!")
    elif result == "0-1":  # Black wins
        black_wins += 1
        print("AI wins!")
    else:
        stalemates += 1
        rla_agent.total_reward -= 30
        print("It's a draw!")

    print("white wins: ", white_wins)
    print("black wins: ", black_wins)
    print("stalemates: ", stalemates)


    board.set_board_fen(chess.STARTING_BOARD_FEN)
    plot_results()
    start_game()



# Main function to start the game based on the selected mode
def start_game():
    display_board()
    print("Starting Player vs RLAI mode...")
    play_pvrla()
    start_game()

# Start the game
start_game()

# Initial board display
display_board()