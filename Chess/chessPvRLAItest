import chess
import tkinter as tk
from tkinter import messagebox
import threading
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random
import time
import matplotlib.pyplot as plt
from matplotlib import figure
from matplotlib import pyplot
import copy
from tkinter import simpledialog



GREEN = f'#395631'
BEIGE = f'#d1b281'


game_numbers = []
total_rewards = []
losses = []
white_wins = 0
black_wins = 0
stalemates = 0
highlighted_square = None  # Global variable to track highlighted square
highlighted_legal_moves = []           # List to store legal moves for the selected piece

uci_click = ""
firstClick = True  # Boolean to track first click

game_number = 1

# Initialize the chess board
board = chess.Board()

# Colors for the chess pieces
white_pieces = {
    chess.PAWN: "♙", chess.KNIGHT: "♘", chess.BISHOP: "♗", chess.ROOK: "♖",
    chess.QUEEN: "♕", chess.KING: "♔"
}
black_pieces = {
    chess.PAWN: "♟", chess.KNIGHT: "♞", chess.BISHOP: "♝", chess.ROOK: "♜",
    chess.QUEEN: "♛", chess.KING: "♚"
}

# Tkinter setup
root = tk.Tk()
root.title("Chess Game")

# Create a canvas to draw the chessboard
canvas = tk.Canvas(root, width=670, height=670)
canvas.pack()

# Define the Deep Q-Network (DQN) model
import torch
import torch.nn as nn

class DQN(nn.Module):
    def __init__(self):
        super(DQN, self).__init__()
        
        # Input layer: 12 * 8 * 8 = 768 (flattened input)
        self.fc1 = nn.Linear(12 * 8 * 8, 2048)  # First hidden layer with 2048 units
        self.fc2 = nn.Linear(2048, 2048)        # Second hidden layer with 2048 units
        self.fc3 = nn.Linear(2048, 1024)        # Third hidden layer with 1024 units
        self.fc4 = nn.Linear(1024, 1024)        # Fourth hidden layer with 1024 units
        self.fc5 = nn.Linear(1024, 512)         # Fifth hidden layer with 512 units
        self.fc6 = nn.Linear(512, 512)          # Sixth hidden layer with 512 units
        self.fc7 = nn.Linear(512, 256)          # Seventh hidden layer with 256 units
        self.fc8 = nn.Linear(256, 256)          # Eighth hidden layer with 256 units
        self.fc9 = nn.Linear(256, 128)          # Ninth hidden layer with 128 units
        self.fc10 = nn.Linear(128, 128)         # Tenth hidden layer with 128 units
        self.fc11 = nn.Linear(128, 64)          # Eleventh hidden layer with 64 units
        self.fc12 = nn.Linear(64, 64)           # Twelfth hidden layer with 64 units
        self.fc13 = nn.Linear(64, 32)           # Thirteenth hidden layer with 32 units
        self.fc14 = nn.Linear(32, 32)           # Fourteenth hidden layer with 32 units
        self.fc15 = nn.Linear(32, 16)           # Fifteenth hidden layer with 16 units
        self.fc16 = nn.Linear(16, 16)           # Sixteenth hidden layer with 16 units
        self.fc17 = nn.Linear(16, 8)            # Seventeenth hidden layer with 8 units
        self.fc18 = nn.Linear(8, 8)             # Eighteenth hidden layer with 8 units
        self.fc19 = nn.Linear(8, 4)             # Nineteenth hidden layer with 4 units
        self.fc20 = nn.Linear(4, 4672)          # Twentieth hidden layer (output) with 4672 units (for possible moves)

        # Dropout layer to prevent overfitting (applied after each hidden layer)
        self.dropout = nn.Dropout(0.3)  # Dropout with 50% probability of zeroing out inputs

    def forward(self, x):
        # Passing the input through all the layers with ReLU activation
        x = torch.relu(self.fc1(x))
        x = self.dropout(x)
        
        x = torch.relu(self.fc2(x))
        x = self.dropout(x)

        x = torch.relu(self.fc3(x))
        x = self.dropout(x)

        x = torch.relu(self.fc4(x))
        x = self.dropout(x)

        x = torch.relu(self.fc5(x))
        x = self.dropout(x)

        x = torch.relu(self.fc6(x))
        x = self.dropout(x)

        x = torch.relu(self.fc7(x))
        x = self.dropout(x)

        x = torch.relu(self.fc8(x))
        x = self.dropout(x)

        x = torch.relu(self.fc9(x))
        x = self.dropout(x)

        x = torch.relu(self.fc10(x))
        x = self.dropout(x)

        x = torch.relu(self.fc11(x))
        x = self.dropout(x)

        x = torch.relu(self.fc12(x))
        x = self.dropout(x)

        x = torch.relu(self.fc13(x))
        x = self.dropout(x)

        x = torch.relu(self.fc14(x))
        x = self.dropout(x)

        x = torch.relu(self.fc15(x))
        x = self.dropout(x)

        x = torch.relu(self.fc16(x))
        x = self.dropout(x)

        x = torch.relu(self.fc17(x))
        x = self.dropout(x)

        x = torch.relu(self.fc18(x))
        x = self.dropout(x)

        x = torch.relu(self.fc19(x))
        x = self.dropout(x)

        # Output layer (no activation function here, raw Q-values)
        x = self.fc20(x)

        return x


class ChessRLAI:
    def __init__(self, model = DQN(), epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995, learning_rate=0.001, gamma=0.99):
        self.model = model
        self.epsilon = epsilon  # Initial epsilon
        self.epsilon_min = epsilon_min  # Minimum value of epsilon
        self.epsilon_decay = epsilon_decay  # Decay factor
        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
        self.loss_fn = nn.MSELoss()  # Loss function for Q-values
        self.gamma = gamma  # Discount factor for future rewards

        self.total_reward = 0.0
        self.total_loss = 0.0


    def reward_of_move(self, move, board) -> float:
        """
        Calculate the reward for a given move. The reward is based on:
        - Material gain or loss
        - Whether the move puts the opponent in check or checkmate
        - 

        Parameters:
            move (chess.Move): The move to evaluate.
            board (chess.Board): The current board state.

        Returns:
            float: The reward for the move.
        """
        reward = 0.0

        if board.is_capture(move) and move is not None:
            captured_piece = board.piece_at(move.to_square)
            # Assign positive reward for capturing a piece
            if captured_piece:
                piece_type = captured_piece.piece_type
                # Assign value based on the piece captured
                if piece_type == chess.PAWN:
                    reward += 1.0
                elif piece_type == piece_type == chess.BISHOP:
                    reward += 3.0
                elif piece_type == piece_type == chess.BISHOP:
                    reward += 5.0
                elif piece_type == chess.ROOK:
                    reward += 7.0
                elif piece_type == chess.QUEEN:
                    reward += 9.0

        # Step 2: Check if the move results in check or checkmate
        board.push(move)
        if board.is_checkmate():
            reward += 50  # Positive large reward for checkmate
        elif board.is_check():
            reward += 10  # Smaller positive reward for check
        board.pop()

        return reward

    def find_best_move_with_q_values(self, board, gamma=0.99, alpha=0.1):
        """
        Calculate the Q-values for each legal move, update the Q-value using Q-learning,
        and return the move with the highest Q-value.

        Parameters:
            board (chess.Board): The current state of the board.
            legal_moves (list): A list of legal moves (chess.Move objects).
            reward (float): The reward for making the move.
            gamma (float): The discount factor for future rewards.
            alpha (float): The learning rate for updating Q-values.

        Returns:
            chess.Move: The move with the highest Q-value (in UCI notation).
        """
        # Initialize a list to store the Q-values for each legal move
        q_values = []

        legal_moves = list(board.legal_moves)

        rand_val = random.random()

        print("rand val: ", rand_val)
        print("epsilon val: ", self.epsilon)

        # Initialize best_move outside the if-else block
        best_move = None

        if rand_val < self.epsilon:
            # Explore: Choose a random legal move
            best_move = random.choice(legal_moves)
            print("using random vals")
        else:
            print("using q-vals")

            # Exploit: Choose the move with the highest Q-value from the model's output
            with torch.no_grad():
                # Loop through each legal move and calculate the Q-value
                for move in legal_moves:
                    # Step 1: Convert the current board state to tensor
                    state_tensor = board_to_tensor(board).unsqueeze(0)  # Add batch dimension

                    # Step 2: Get the Q-values for the current state
                    current_q_values = self.model(state_tensor).squeeze(0)

                    # Step 3: Simulate the move by copying the board and making the move on the clone
                    cloned_board = copy.deepcopy(board)  # Clone the board
                    cloned_board.push(move)  # Make the move on the cloned board

                    # Step 4: Convert the new state after the move to tensor
                    next_state_tensor = board_to_tensor(cloned_board).unsqueeze(0)  # Get the next state tensor after the move

                    # Step 5: Get the Q-values for the next state
                    next_q_values = self.model(next_state_tensor).squeeze(0)

                    # Step 6: Calculate the maximum Q-value for the next state
                    max_next_q_value = torch.max(next_q_values).item()

                    # Step 7: Find the index of the move in the model's Q-value predictions
                    move_index = legal_moves.index(move)

                    # Step 8: Get the current Q-value for the move
                    current_q_value = current_q_values[move_index].item()

                    # Step 9: Calculate the new Q-value using the Q-learning equation
                    updated_q_value = current_q_value + alpha * (self.reward_of_move(move, board) + gamma * max_next_q_value - current_q_value)

                    # Append the updated Q-value to the list
                    q_values.append(updated_q_value)

                # Convert q_values list to tensor
                q_values_tensor = torch.tensor(q_values)

                # Step 10: Calculate the loss (using MSE) and update Q-value
                # We should update the Q-value for the move that was selected
                updated_q_value_tensor = torch.tensor([updated_q_value])  # Convert updated_q_value to tensor

                # Ensure q_values_tensor is a tensor of shape (N,)
                loss = self.loss_fn(q_values_tensor, updated_q_value_tensor)

                # Update the total loss
                self.total_loss += loss.item()

                # Step 11: Find the move with the highest Q-value (from q_values)
                best_move_index = np.argmax(q_values)
                best_move = legal_moves[best_move_index]

                # Track cumulative reward (this should be done only after best_move is assigned)
                self.total_reward += self.reward_of_move(best_move, board)

        # Decay epsilon after each move
        self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)

        return best_move
    


def plot_results():
    """
    Creates a 1x2 grid of subplots:
    - First subplot: Total Reward vs Game Number
    - Second subplot: Pie chart for black wins, white wins, and stalemates (only if count > 0)
    """
    global white_wins, black_wins, stalemates, game_numbers, total_rewards, losses
    
    # Create a 1x2 grid of subplots
    fig, ax = plt.subplots(1, 2, figsize=(14, 6))
    
    # First subplot: Total Reward vs Game Number
    ax[0].plot(game_numbers, total_rewards, label="Total Reward", color='b')
    ax[0].set_xlabel("Game Number")
    ax[0].set_ylabel("Total Reward")
    ax[0].set_title("Total Reward vs Game Number")
    ax[0].grid(True)
    
    # Second subplot: Pie chart for black wins, white wins, and stalemates
    pie_data = [black_wins, white_wins, stalemates]
    labels = ['Black Wins', 'White Wins', 'Stalemates']
    
    # Only plot the pie chart if any value > 0
    if any(count > 0 for count in pie_data):
        ax[1].pie(pie_data, labels=labels, autopct='%1.1f%%', startangle=90, colors=['black', 'white', 'gray'])
        ax[1].set_title("Black Wins vs White Wins vs Stalemates")
    else:
        ax[1].text(0.5, 0.5, 'No data to plot', ha='center', va='center', fontsize=12)

    # Adjust layout to avoid overlap
    plt.tight_layout()

    # Show the plot
    plt.show()

# Function to handle click events
def on_click(event):
    global highlighted_square, highlighted_legal_moves, firstClick, uci_click, turn  # Access global variables

    # Calculate the row and column of the clicked square based on the canvas click
    col = event.x // 80
    row = event.y // 80

    # Convert to chess notation (e.g., "a1", "h8") based on the reversed orientation
    chess_col = chr(col + 97)  # Convert column to letter ('a' to 'h')
    chess_row = 8 - row        # Convert row to number ('1' to '8') based on flipped orientation
    clicked_square = chess_col + str(chess_row)   
    # Convert the file (char) to an index (0-7)
    file_index = ord(chess_col) - 97  # 'a' -> 0, 'b' -> 1, ..., 'h' -> 7
    
    # Convert the rank (char) to an integer (1 -> 0, 2 -> 1, ..., 8 -> 7)
    rank_index = int(chess_row) - 1  # "1" -> 0, "2" -> 1, ..., "8" -> 7
                
    # Use chess.square to convert (file_index, rank_index) to the square index
    square = chess.square(file_index, rank_index)
    

    # Print the chessboard notation (e.g., "a1", "h8") for debugging
    print(f"Clicked on square: {clicked_square}")
    print(f"Square index: {square}")

    # Check if there's a piece on the clicked square
    piece = board.piece_at(chess.square(col, 7 - row))

    # If it's the first click and the user selects a piece
    if firstClick:
        if piece:
            # Get legal moves for the clicked square (if it's a piece)
            highlighted_legal_moves = get_legal_moves(board, clicked_square)
            highlighted_square = (row, col)  # Highlight the selected piece's position

            # Debugging: Check if the highlighted_legal_moves is being populated
            print(f"Highlighted legal moves: {highlighted_legal_moves}")
            uci_click = clicked_square
            print("uci clicked = ", uci_click)
            # After the first click, switch to second click mode
            firstClick = False
        else:
            print("No piece at clicked square. Please select a piece.")
    
    else:
        # If it's the second click, check if the square is a legal move
        print("legal moves array: ", highlighted_legal_moves)
        if square in highlighted_legal_moves:
            print("Legal move found!")
            uci_click += clicked_square
            print("uci clicked = ", uci_click)
            makePlayerMove(uci_click)  # Execute the move 
            firstClick = True   
            uci_click = ""
            highlighted_square = None
            highlighted_legal_moves = []
            turn.set(1)
            # Reset to first click mode after move
        elif piece:
            print("Clicked on a piece: " + str(piece) + ". At index: " + str(square) + ". Clicked on:" + clicked_square)
            highlighted_square = None
            highlighted_legal_moves = []
            firstClick = True  # Go back to selecting a new piece
        else:
            print("Clicked on an invalid square or no piece to move. Clicked on:" + str(square))
            # Optionally, reset the highlighted square and legal moves
            highlighted_square = None
            highlighted_legal_moves = []
            firstClick = True  # Go back to selecting a new piece


    # Redraw the board with the updated highlighted square and legal moves
    canvas.delete("all")  # Clear the canvas
    display_board()        # Redraw the board with updated highlights


# Function to get legal moves for a given square (in UCI notation)
def get_legal_moves(board, clicked_square):
    legal_moves = []
    moves = board.legal_moves  # This should be a set of LegalMove objects
    
    for move in moves:
        # Convert the move to UCI notation (e.g., "e2e4")
        uci_move = move.uci()
        
        # Check if the first two characters of UCI move match the clicked square
        if uci_move[:2] == clicked_square:
            # Extract the destination square in UCI format (e.g., "e4")
            destination_square_uci = uci_move[2:4]
            file_char = destination_square_uci[0]  # e.g., "e"
            rank_char = destination_square_uci[1]  # e.g., "4"
            
                        # Convert the file (char) to an index (0-7)
            file_index = ord(file_char) - 97  # 'a' -> 0, 'b' -> 1, ..., 'h' -> 7
            
            # Convert the rank (char) to an integer (1 -> 0, 2 -> 1, ..., 8 -> 7)
            rank_index = int(rank_char) - 1  # "1" -> 0, "2" -> 1, ..., "8" -> 7
                        
            # Use chess.square to convert (file_index, rank_index) to the square index
            destination_square = chess.square(file_index, rank_index)
            
            legal_moves.append(destination_square)
    
    return legal_moves


# Function to display the board in Tkinter
def display_board():
    global highlighted_legal_moves
    square_size = 80
    for row in range(8):
        for col in range(8):
            # Determine the color of the square
            color = GREEN if (row + col) % 2 == 0 else BEIGE
            canvas.create_rectangle(col * square_size, row * square_size,
                                    (col + 1) * square_size, (row + 1) * square_size,
                                    fill=color)

            # Highlight the selected square if it matches
            if highlighted_square == (row, col):
                canvas.create_rectangle(col * square_size, row * square_size,
                                        (col + 1) * square_size, (row + 1) * square_size,
                                        outline="red", width=3)  # Red border for selected square

            # Highlight the legal moves (blue border)
            if chess.square(col, 7- row) in highlighted_legal_moves:
                print(f"Legal move found: {chess.square(col, 7 - row)}")
                canvas.create_rectangle(col * square_size, row * square_size,
                                        (col + 1) * square_size, (row + 1) * square_size,
                                        outline="blue", width=2)  # Blue border for legal moves

            # Get the piece at the current square
            piece = board.piece_at(chess.square(col, 7 - row))  # Flip y-axis for correct orientation
            if piece:
                piece_char = ""
                if piece.color == chess.WHITE:
                    piece_char = white_pieces.get(piece.piece_type, "")
                else:
                    piece_char = black_pieces.get(piece.piece_type, "")
                canvas.create_text(col * square_size + square_size / 2,
                                   row * square_size + square_size / 2,
                                   text=piece_char, font=("Times New Roman", 36))

            # Display the square number (0 to 63) in the center of the square
            square_number = chess.square(col, 7 - row)  # Get the square number from (col, row)
            canvas.create_text(col * square_size + square_size / 2,
                               row * square_size + square_size / 2,
                               text=str(square_number), font=("Times New Roman", 12, "bold"), fill="black")

    # Draw row labels (1-8) on the left side of the board
    for row in range(8):
        canvas.create_text(8 * square_size + 10, row * square_size + square_size / 2,  # Right side positioning
                           text=str(8 - row), font=("Times New Roman", 14))

    # Draw column labels (a-h) at the bottom of the board
    for col in range(8):
        canvas.create_text(col * square_size + square_size / 2,
                           8 * square_size + 10,  # Position below the board
                           text=chr(ord('a') + col), font=("Times New Roman", 14))
        


def makePlayerMove(move_uci):
    global board
    try:
        # Apply the move using UCI notation
        move = chess.Move.from_uci(move_uci)
        piece = board.piece_at(move.from_square)
        # Check for promotion
        if piece and piece.piece_type == chess.PAWN and \
            ((move.to_square < 65 and move.to_square > 55) or (move.to_square < 8 and move.to_square > -1)):  
            # Pawn moved to promotion rank (1st or 8th rank)
            print("promoting")
            
            # Ask for promotion choice using Tkinter dialog
            root = tk.Tk()
            root.withdraw()  # Hide the main window
            promotion_choice = simpledialog.askstring("Pawn Promotion", 
                                                      "Promote to (Q [Queen], R [Rook], B [Bishop], N [Knight]):", 
                                                      parent=root)
            # Map promotion choice to the correct piece type
            piece_map = {'Q': chess.QUEEN, 'R': chess.ROOK, 'B': chess.BISHOP, 'N': chess.KNIGHT}
            
            if promotion_choice in piece_map:  # Validate input
                promotion_piece = piece_map[promotion_choice]  # Get the piece type constant
                print("promotion piece: ", promotion_piece)
                
                # Create the promoted move
                move = chess.Move(move.from_square, move.to_square, promotion=promotion_piece)
                print("new promotion move: ", move)
                print(f"Pawn promoting to {promotion_choice}")
        if move in board.legal_moves:
            board.push(move)
            print(f"Move made: {move_uci}")
            display_board()
        else:
            print("Invalid move!")
    except ValueError:
        print("Invalid UCI format or move.")

# Function to handle a player's move from the console
def make_move(move_uci):
    global board
    try:
        # Apply the move using UCI notation
        move = chess.Move.from_uci(move_uci)
        if move in board.legal_moves:
            board.push(move)
            print(f"Move made: {move_uci}")
            display_board()
        else:
            print("Invalid move!")
    except ValueError:
        print("Invalid UCI format or move.")


def board_to_tensor(board):
    # Create a tensor to represent the board state (12 layers, 8x8 grid)
    tensor = torch.zeros((12, 8, 8), dtype=torch.float32)  # 12 layers (6 piece types for white and black)

    # Iterate over each square on the chessboard (64 squares in total)
    for square in range(64):
        piece = board.piece_at(square)

        if piece:  # If there is a piece at this square
            row, col = divmod(square, 8)  # Get the row and column from the square number

            # Determine the piece type and color
            piece_type = piece.piece_type
            color = piece.color

            # Determine the index in the tensor: for white pieces, use positive values
            # For black pieces, use negative values and the index to determine layer
            layer = piece_type - 1  # piece_type 1 corresponds to layer 0, etc.

            if color == chess.BLACK:
                layer += 6  # Black pieces are in the second half of the tensor (layers 6 to 11)

            # Add the piece's value to the appropriate position in the tensor
            tensor[layer, row, col] = 1  # We are simply marking the presence of a piece, so use a value of 1

    # Flatten the tensor to match the input shape expected by the neural network
    tensor = tensor.view(-1)  # Flatten the 12x8x8 tensor into a 768-length vector
    return tensor

def print_board_tensor():
    tensor = board_to_tensor(board)
    print("Tensor shape:", tensor.shape)
    print(tensor)

rla_agent = ChessRLAI(model=DQN())
turn = tk.IntVar(value=0)

# Function to handle the Player vs RLAI mode
def play_pvrla():
    global game_number
    print("play_pvrla started")
    while not board.is_game_over():
        print("turn before display_board: ", turn.get())
        display_board()
        root.update_idletasks()

        print(" ")
        print("Legal moves: ")
        print(board.legal_moves)

        if turn.get() == 0:  # White Player's Turn
            print("White Player's Turn = ", turn.get())
            canvas.bind("<Button-1>", on_click)  # Bind the event to on_click
            root.wait_variable(turn)  # Wait for the player to make a move
            canvas.unbind("<Button-1>")  # Unbind the event after the move is made
            print("Turn after White's move: ", turn.get())
            if turn.get() == 0:
                print("Turn didn't change after White's move, check on_click.")
            turn.set(1)  # Change to Black's turn after player's move
            print("Turn after setting to Black: ", turn.get()) 
            root.update_idletasks()  # Force UI updates
            root.after(100)  # Add a small delay to ensure the turn is updated
            print("turn after setting to Black: ", turn.get())

        elif turn.get() == 1:  # Black AI's Turn
            print("Black AI's Turn = ", not turn.get())
            action = rla_agent.find_best_move_with_q_values(board)  # Choose an action based on the state
            print("AI action: ", action)
            legal_moves = list(board.legal_moves)

            # Make sure the action is one of the legal moves
            if action in legal_moves:
                move = action  # Get the corresponding move from legal_moves
                print(f"Black AI plays: {move}")
                make_move(move.uci())  # Apply the move
                display_board()  # Ensure the board is redrawn after the move
                root.update_idletasks()  # Force UI updates
                root.after(20)  # Non-blocking way to add a delay for viewing purposes
            else:
                continue

            turn.set(0)  # Change to White's turn after AI's move

        # Introduce a small delay for better viewing of the moves
        root.after(20)  # Non-blocking way to add a delay for viewing purposes

    # After the game ends, record total reward and loss
    game_numbers.append(game_number)
    total_rewards.append(rla_agent.total_reward)
    losses.append(rla_agent.total_loss)


    # Once the game is over, print the result
    print("Game Over!")
    print(f"Result: {board.result()}")

    result = board.result()
    if result == "1-0":  # White wins
        white_wins += 1
        print("Player wins!")
    elif result == "0-1":  # Black wins
        black_wins += 1
        print("AI wins!")
    else:
        stalemates += 1
        print("It's a draw!")

    print("white wins: ", white_wins)
    print("black wins: ", black_wins)
    print("stalemates: ", stalemates)

    game_number += 1

    board.set_board_fen(chess.STARTING_BOARD_FEN)
    start_game()



# Main function to start the game based on the selected mode
def start_game():
    display_board()
    print("Starting Player vs RLAI mode...")
    play_pvrla()
    start_game()

# Start the game
start_game()

# Initial board display
display_board()