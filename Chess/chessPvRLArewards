import chess
import tkinter as tk
from tkinter import messagebox
import threading
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random
import time
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure
import copy
from tkinter import simpledialog
import os
import random
import torch
import chess
import chess.engine
import numpy as np
from tqdm import tqdm
import torch.nn as nn
import torch.optim as optim
import torch.utils.data as data
import seaborn as sns
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_score, recall_score, f1_score
import matplotlib.pyplot as plt

import json



GREEN = f'#395631'
BEIGE = f'#d1b281'


game_numbers = []
game_numbers.append(0.0)
total_rewards = []
sum_rewards = []
sum_rewards.append(0.0)
turn_rewards = []
turn_numbers =[]
average_game_q_values = []
average_game_q_values.append(0.0)
best_move_q_value = 0.0
turn_q_values = []
average_q_value = 0.0
turn_number = 0.0
white_wins = 0
black_wins = 0
stalemates = 0
highlighted_square = None  # Global variable to track highlighted square
highlighted_legal_moves = []           # List to store legal moves for the selected piece
game_total_rewards = []  # List to store total rewards for each game
game_total_rewards.append(0.0)

stockBestQ = 0.0
stockBestVals = []

fig = None
ax = None

uci_click = ""
firstClick = True  # Boolean to track first click

plt.ion()

# Initialize the chess board
board = chess.Board()

# Colors for the chess pieces
white_pieces = {
    chess.PAWN: "♙", chess.KNIGHT: "♘", chess.BISHOP: "♗", chess.ROOK: "♖",
    chess.QUEEN: "♕", chess.KING: "♔"
}
black_pieces = {
    chess.PAWN: "♟", chess.KNIGHT: "♞", chess.BISHOP: "♝", chess.ROOK: "♜",
    chess.QUEEN: "♛", chess.KING: "♚"
}

# Tkinter setup
root = tk.Tk()
root.title("Chess Game")

# Create a canvas to draw the chessboard
canvas = tk.Canvas(root, width=670, height=670)
canvas.pack()

# Define the Deep Q-Network (DQN) model
import torch
import torch.nn as nn



class DQN(nn.Module):
    def __init__(self):
        super(DQN, self).__init__()
        
        # Input layer: 12 * 8 * 8 = 768 (flattened input)
        self.fc1 = nn.Linear(12 * 8 * 8, 2048)  # First hidden layer with 2048 units
        self.fc2 = nn.Linear(2048, 2048)        # Second hidden layer with 2048 units
        self.fc3 = nn.Linear(2048, 1024)        # Third hidden layer with 1024 units
        self.fc4 = nn.Linear(1024, 1024)        # Fourth hidden layer with 1024 units
        self.fc5 = nn.Linear(1024, 512)         # Fifth hidden layer with 512 units
        self.fc6 = nn.Linear(512, 512)          # Sixth hidden layer with 512 units
        self.fc7 = nn.Linear(512, 256)          # Seventh hidden layer with 256 units
        self.fc8 = nn.Linear(256, 256)          # Eighth hidden layer with 256 units
        self.fc9 = nn.Linear(256, 128)          # Ninth hidden layer with 128 units
        self.fc10 = nn.Linear(128, 128)         # Tenth hidden layer with 128 units
        self.fc11 = nn.Linear(128, 64)          # Eleventh hidden layer with 64 units
        self.fc12 = nn.Linear(64, 64)           # Twelfth hidden layer with 64 units
        self.fc13 = nn.Linear(64, 32)           # Thirteenth hidden layer with 32 units
        self.fc14 = nn.Linear(32, 32)           # Fourteenth hidden layer with 32 units
        self.fc15 = nn.Linear(32, 16)           # Fifteenth hidden layer with 16 units
        self.fc16 = nn.Linear(16, 16)           # Sixteenth hidden layer with 16 units
        self.fc17 = nn.Linear(16, 8)            # Seventeenth hidden layer with 8 units
        self.fc18 = nn.Linear(8, 8)             # Eighteenth hidden layer with 8 units
        self.fc19 = nn.Linear(8, 4)             # Nineteenth hidden layer with 4 units
        self.fc20 = nn.Linear(4, 4672)  # Output layer with 4672 units (for possible moves)

        # Dropout layer to prevent overfitting (applied after each hidden layer)
        self.dropout = nn.Dropout(0.3)  # Dropout with 30% probability of zeroing out inputs

    def forward(self, x):
        # Passing the input through all the layers with ReLU activation
        x = torch.relu(self.fc1(x))
        x = self.dropout(x)
        
        x = torch.relu(self.fc2(x))
        x = self.dropout(x)

        x = torch.relu(self.fc3(x))
        x = self.dropout(x)

        x = torch.relu(self.fc4(x))
        x = self.dropout(x)

        x = torch.relu(self.fc5(x))
        x = self.dropout(x)

        x = torch.relu(self.fc6(x))
        x = self.dropout(x)

        x = torch.relu(self.fc7(x))
        x = self.dropout(x)

        x = torch.relu(self.fc8(x))
        x = self.dropout(x)

        x = torch.relu(self.fc9(x))
        x = self.dropout(x)

        x = torch.relu(self.fc10(x))
        x = self.dropout(x)

        x = torch.relu(self.fc11(x))
        x = self.dropout(x)

        x = torch.relu(self.fc12(x))
        x = self.dropout(x)

        x = torch.relu(self.fc13(x))
        x = self.dropout(x)

        x = torch.relu(self.fc14(x))
        x = self.dropout(x)

        x = torch.relu(self.fc15(x))
        x = self.dropout(x)

        x = torch.relu(self.fc16(x))
        x = self.dropout(x)

        x = torch.relu(self.fc17(x))
        x = self.dropout(x)

        x = torch.relu(self.fc18(x))
        x = self.dropout(x)

        x = self.fc19(x)  # No activation for the last hidden layer
        x = self.fc20(x)  # Output layer with dynamic action space size
        return x

class ModelTrainer:
    def __init__(self, model=DQN(), loss_fn=nn.MSELoss(), lr=0.001):
        self.model = model
        self.optimizer = optim.Adam(model.parameters(), lr=lr)
        self.loss_fn = loss_fn

    def load_data(self):
        if os.path.exists('chess_games.pt'):
            games = torch.load('chess_games.pt')

            # Debug: Print structure of the first game to verify
            print("Structure of first game:", games[0])  # Should be a tuple with board tensor and move
            
            inputs = []
            targets = []
            move_to_index = {}  # To map each unique move to an index
            index = 0

            # Process the games
            for game in games:
                if isinstance(game, tuple) and len(game) == 2:
                    board_tensor, move = game  # Unpack the tuple correctly
                    inputs.append(board_tensor)

                    # Get the UCI string of the move and map it to an index
                    move_uci = move.uci()
                    if move_uci not in move_to_index:
                        move_to_index[move_uci] = index
                        index += 1
                    
                    targets.append(move_to_index[move_uci])  # Use the index of the move
                else:
                    print("Unexpected format for game:", game)

            # Convert lists to tensors
            inputs = torch.stack(inputs)
            targets = torch.tensor(targets)  # Targets are now move indices
            return inputs, targets, len(move_to_index)
        else:
            raise FileNotFoundError("chess_games.pt not found!")

    def compute_accuracy(self, outputs, targets):
        _, predicted = torch.max(outputs, 1)  # Get the index of the highest probability
        correct = (predicted == targets).sum().item()  # Count correct predictions
        accuracy = correct / len(targets)  # Compute accuracy
        return accuracy

    def compute_metrics(self, outputs, targets):
        _, predicted = torch.max(outputs, 1)
        precision = precision_score(targets.cpu(), predicted.cpu(), average='weighted', zero_division=1)
        recall = recall_score(targets.cpu(), predicted.cpu(), average='weighted', zero_division=1)
        f1 = f1_score(targets.cpu(), predicted.cpu(), average='weighted', zero_division=1)
        return precision, recall, f1

    def plot_loss_curve(self, epoch_losses):
        plt.plot(range(1, len(epoch_losses) + 1), epoch_losses, marker='o')
        plt.title('Training Loss Over Epochs')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.grid(True)
        plt.show()
        input("Press 'done' to continue to the next graph...")

    def plot_gradients(self):
        gradients = []
        for param in self.model.parameters():
            if param.grad is not None:
                gradients.append(param.grad.abs().mean().item())
        plt.plot(gradients)
        plt.title('Average Gradient Magnitudes')
        plt.xlabel('Parameter Index')
        plt.ylabel('Average Gradient Magnitude')
        plt.show()
        input("Press 'done' to continue to the next graph...")

    def plot_output_distribution(self, outputs):
        probs = torch.softmax(outputs, dim=1)  # Apply softmax to get probabilities
        probs = probs.cpu().detach().numpy()  # Convert to numpy for plotting
        plt.hist(probs.flatten(), bins=50)  # Plot a histogram of probabilities
        plt.title('Output Probability Distribution')
        plt.xlabel('Probability')
        plt.ylabel('Frequency')
        plt.show()
        input("Press 'done' to continue to the next graph...")

    def plot_weight_histograms(self):
        for name, param in self.model.named_parameters():
            if 'weight' in name:
                plt.clf()
                plt.hist(param.detach().cpu().numpy().flatten(), bins=50)
                plt.title(f'Weight Histogram - {name}')
                plt.xlabel('Weight Value')
                plt.ylabel('Frequency')
                plt.show()
                input("Press 'done' to continue to the next graph...")

    def plot_confusion_matrix(self, outputs, targets):
        _, predicted = torch.max(outputs, 1)
        cm = confusion_matrix(targets.cpu(), predicted.cpu())
        plt.figure(figsize=(10, 8))
        sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=range(cm.shape[0]), yticklabels=range(cm.shape[0]))
        plt.xlabel('Predicted')
        plt.ylabel('True')
        plt.title('Confusion Matrix')
        plt.show()
        input("Press 'done' to continue to the next graph...")

    def plot_metric_curve(self, metric_name, metric_values):
        plt.plot(range(1, len(metric_values) + 1), metric_values, marker='o')
        plt.title(f'{metric_name} Over Epochs')
        plt.xlabel('Epoch')
        plt.ylabel(metric_name)
        plt.grid(True)
        plt.show()
        input("Press 'done' to continue to the next graph...")

    def train_model(self, epochs=400, batch_size=400, lr=0.001):
        # Load data
        inputs, targets, indexMoveLength = self.load_data()

        # Create a DataLoader for batching
        dataset = data.TensorDataset(inputs, targets)
        dataloader = data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

        # Change to CrossEntropyLoss for classification
        self.loss_fn = nn.CrossEntropyLoss()  

        # Store metrics
        epoch_losses = []
        epoch_accuracies = []
        epoch_precisions = []
        epoch_recalls = []
        epoch_f1_scores = []

        # Training loop with tqdm for progress bars
        self.model.train()
        for epoch in range(epochs):
            running_loss = 0.0
            epoch_accuracy = 0.0
            epoch_precision = 0.0
            epoch_recall = 0.0
            epoch_f1 = 0.0

            for i, (inputs_batch, targets_batch) in tqdm(enumerate(dataloader), total=len(dataloader), desc=f"Epoch {epoch+1}/{epochs}", ncols=100, unit="batch"):
                # Zero gradients
                self.optimizer.zero_grad()

                # Forward pass through the model
                outputs = self.model(inputs_batch)

                # Ensure targets are 1D class indices (not one-hot encoded)
                targets_batch = targets_batch.view(-1)  # Flatten targets to shape (batch_size,)

                # Compute the loss
                loss = self.loss_fn(outputs, targets_batch)

                # Compute accuracy, precision, recall, and F1-score
                accuracy = self.compute_accuracy(outputs, targets_batch)
                precision, recall, f1 = self.compute_metrics(outputs, targets_batch)

                # Backpropagation
                loss.backward()

                # Optimize the model
                self.optimizer.step()

                running_loss += loss.item()
                epoch_accuracy += accuracy
                epoch_precision += precision
                epoch_recall += recall
                epoch_f1 += f1

            # Average loss and metrics for the epoch
            avg_loss = running_loss / len(dataloader)
            avg_accuracy = epoch_accuracy / len(dataloader)
            avg_precision = epoch_precision / len(dataloader)
            avg_recall = epoch_recall / len(dataloader)
            avg_f1 = epoch_f1 / len(dataloader)

            epoch_losses.append(avg_loss)
            epoch_accuracies.append(avg_accuracy)
            epoch_precisions.append(avg_precision)
            epoch_recalls.append(avg_recall)
            epoch_f1_scores.append(avg_f1)

            print(f"Epoch {epoch+1}/{epochs}, Avg. Loss: {avg_loss:.4f}, Accuracy: {avg_accuracy:.4f}, Precision: {avg_precision:.4f}, Recall: {avg_recall:.4f}, F1: {avg_f1:.4f}")

        # Optionally, plot the loss and other metrics
        plt.clf()
        self.plot_loss_curve(epoch_losses)
        plt.clf()
        self.plot_metric_curve('Accuracy', epoch_accuracies)
        plt.clf()
        self.plot_metric_curve('Precision', epoch_precisions)
        plt.clf()
        self.plot_metric_curve('Recall', epoch_recalls)
        plt.clf()
        self.plot_metric_curve('F1 Score', epoch_f1_scores)
        plt.clf()

        # Additional plots
        self.plot_gradients()
        plt.clf()
        self.plot_output_distribution(outputs)  # After training ends, use the final outputs
        plt.clf()
        self.plot_weight_histograms()
        plt.clf()

    def pretrain_and_save_model(self):
        # Pre-train the model
        self.train_model(epochs=400, batch_size=100, lr=0.001)

        # Save the trained model
        torch.save(self.model.state_dict(), 'chess_ai_model_v1.pth')
        print("Model saved as 'chess_ai_model_v1.pth'")
    

class ChessRLAI:
    def __init__(self, model = DQN(), epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995, learning_rate=0.001, gamma=0.99):
        self.model = model
        self.epsilon = epsilon  # Initial epsilon
        self.epsilon_min = epsilon_min  # Minimum value of epsilon
        self.epsilon_decay = epsilon_decay  # Decay factor
        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
        self.loss_fn = nn.MSELoss()  # Loss function for Q-values
        self.gamma = gamma  # Discount factor for future rewards

        self.total_reward = 0.0
        self.sum_reward = 0.0
        self.total_loss = 0.0
        self.turn_reward = 0.0

        self.board_evaluations = {}

        self.load_pretrained_weights(r"C:\Users\Grace\Documents\GitHub\chessAI\chess_ai_model_v1.pth")

        self.stockfishpath = (r"C:\Users\Grace\Documents\GitHub\chessAI\Chess\Pre-train\stockfish-windows-x86-64-avx2\stockfish\stockfish-windows-x86-64-avx2.exe")


    def load_model(model):
        filename = r"C:\Users\Grace\Documents\GitHub\chessAI\chess_ai_model_v1.pth"
        model.load_state_dict(torch.load(filename))
        model.eval()  # Set the model to evaluation mode
        print(f"Model loaded from {filename}")


    def load_pretrained_weights(self, path):
        self.model.load_state_dict(torch.load(path))

    def reward_of_move(self, move, board) -> float:
        """
        Calculate the reward for a given move on a chess board assuming Black is playing.
        The reward is based on Stockfish evaluation, with positive rewards favoring Black
        and negative rewards favoring White.

        Parameters:
            move (chess.Move): The move to evaluate.
            board (chess.Board): The current state of the board.

        Returns:
            float: The reward for the move. Positive reward favors Black, negative favors White.
        """
        # Use a unique identifier for the board state (e.g., the FEN)
        board_fen = board.fen()

        # If we've already evaluated this board, use the cached result
        if board_fen in self.board_evaluations:
            return self.board_evaluations[board_fen]

        # Clone the current board and make the move
        cloned_board = board.copy()
        cloned_board.push(move)

        # Get the evaluation after the move using Stockfish
        with chess.engine.SimpleEngine.popen_uci(self.stockfishpath) as engine:
            # Get the evaluation of the current position
            info = engine.analyse(board, chess.engine.Limit(time=1.0))  # Analyze for a small time
            score = info['score']

            # Check if the score is a "mate" score (which indicates checkmate or stalemate)
            if score.is_mate():
                # If it's a mate score, return a large negative reward for Black's victory
                if score.mate() > 0:  # White to move and mate (Black loses)
                    reward = -100  # Positive reward for White's victory
                else:  # Black to move and mate (Black wins)
                    reward = 100  # Negative reward for Black's victory
            else:
                # If it's not a mate score, return the centipawn value
                    reward = -score.relative.score() / 100.0  # Centipawn value is negated to favor Black

        # Cache the evaluation for the current board state
        self.board_evaluations[board_fen] = reward
        
        return reward
    

    def get_best_move(self, board, time_limit: float = 2.0):
        """
        Determines the best move from the current board state using Stockfish.
        
        :param board: The current chess board state (chess.Board object).
        :param stockfish_path: Path to the Stockfish engine executable.
        :param time_limit: Time in seconds for Stockfish to think (default is 1.0 second).
        :return: The best move (chess.Move object).
        """
        
        # Start Stockfish engine
        with chess.engine.SimpleEngine.popen_uci(self.stockfishpath) as engine:
            # Get the best move by analyzing the current board state
            result = engine.play(board, chess.engine.Limit(time=time_limit))
            
            # The best move chosen by Stockfish
            best_move = result.move
        
        return best_move
    

    def getQVals(self, board, move, legal_moves, alpha, gamma):
        """
        Calculate the Q-value for a given move on the current board state using the Q-learning equation.

        Parameters:
            board (chess.Board): The current chess board state.
            move (chess.Move): The move to evaluate.
            legal_moves (list): List of legal moves in the current position.
            alpha (float): Learning rate for Q-value update.
            gamma (float): Discount factor for future rewards.

        Returns:
            float: The updated Q-value for the given move.
        """
        # Step 1: Convert the current board state to tensor
        state_tensor = board_to_tensor(board).unsqueeze(0)  # Add batch dimension

        # Step 2: Get the Q-values for the current state
        current_q_values = self.model(state_tensor).squeeze(0)

        # Step 3: Simulate the move by copying the board and making the move on the clone
        cloned_board = copy.deepcopy(board)  # Clone the board
        cloned_board.push(move)  # Make the move on the cloned board

        # Step 4: Convert the new state after the move to tensor
        next_state_tensor = board_to_tensor(cloned_board).unsqueeze(0)  # Get the next state tensor after the move

        # Step 5: Get the Q-values for the next state
        next_q_values = self.model(next_state_tensor).squeeze(0)

        # Step 6: Calculate the maximum Q-value for the next state
        max_next_q_value = torch.max(next_q_values).item()

        # Step 7: Find the index of the move in the model's Q-value predictions
        move_index = legal_moves.index(move)

        # Step 8: Get the current Q-value for the move
        current_q_value = current_q_values[move_index].item()

        # Step 9: Calculate the new Q-value using the Q-learning equation
        updated_q_value = current_q_value + alpha * (self.reward_of_move(move, board) + gamma * max_next_q_value - current_q_value)

        return updated_q_value



    def find_best_move_with_q_values(self, board, gamma=0.99, alpha=0.1):
        global best_move_q_value, stockBestVals
        """
        Calculate the Q-values for each legal move, update the Q-value using Q-learning,
        and return the move with the highest Q-value.
        """
        # Initialize a list to store the Q-values for each legal move
        q_values = []

        legal_moves = list(board.legal_moves)

        rand_val = random.random()
        print("rand val: ", rand_val)
        print("epsilon val: ", self.epsilon)

        # Initialize best_move outside the if-else block
        best_move = None
        if rand_val < self.epsilon:
            # Explore: Choose a random legal move
            best_move = random.choice(legal_moves)
            print("using random vals")

            # Calculate Q-value for the random move, even though it's not used in decision making
            state_tensor = board_to_tensor(board).unsqueeze(0)  # Convert the current board state to tensor
            current_q_values = self.model(state_tensor).squeeze(0)  # Get the Q-values for the current state

            # Find the index of the random move in the legal_moves list
            movedex = legal_moves.index(best_move)

            # Get the Q-value for the random move
            random_move_q_value = current_q_values[movedex].item()
            print(f"Q-value of random move {best_move}: {random_move_q_value}")

            # Since it's a random move, we can assign a default Q-value (e.g., 0) for it
            best_move_q_value = random_move_q_value

            stockMove = self.get_best_move(board)
            
            stockMove_q_value = self.reward_of_move(stockMove, board)

            stockBestVals.append(stockMove_q_value)

            self.total_reward += self.reward_of_move(best_move, board)
        else:
            print("using q-vals")

            # Exploit: Choose the move with the highest Q-value from the model's output
            with torch.no_grad():
                # Loop through each legal move and calculate the Q-value
                for move in legal_moves:
                    updated_q_value = self.getQVals(board, move, legal_moves, alpha, gamma)  # Call getQvals

                    # Append the updated Q-value to the list
                    q_values.append(updated_q_value)

                # Convert q_values list to tensor
                q_values_tensor = torch.tensor(q_values)

                # Step 10: Find the move with the highest Q-value (from q_values)
                best_move_index = np.argmax(q_values)
                best_move = legal_moves[best_move_index]

                best_move_q_value = q_values[best_move_index]

                self.turn_reward = self.reward_of_move(best_move, board)

                self.total_reward += self.reward_of_move(best_move, board)

                self.sum_reward += self.reward_of_move(best_move, board)
            
            stockMove = self.get_best_move(board)
            
            stockMove_q_value = self.getQVals(board, stockMove, legal_moves, alpha, gamma)

            stockBestVals.append(stockMove_q_value)
                
        turn_q_values.append(best_move_q_value)

        # Decay epsilon after each move
        self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)
        
        return best_move
    


def plot_results():
    """
    Creates or updates a 1x3 grid of subplots:
    - First subplot: Total Reward vs Game Number with losses
    - Second subplot: Pie chart for black wins, white wins, and stalemates (only if count > 0)
    - Third subplot: Total Reward vs Turns
    """
    global white_wins, black_wins, stalemates, game_numbers, total_rewards, fig, ax, turn_q_values, average_game_q_values
    
    # Create a 1x3 grid of subplots if not created yet
    if fig is None or ax is None:
        fig, ax = plt.subplots(2, 3, figsize=(10, 8)) 

    #print("game numbers array: ", game_numbers)
    print("turn numbers array: ", turn_numbers)
    #print("total rewards array: ", total_rewards)
    #print("game total rewards array: ", game_total_rewards)
    print("turn rewards array: ", turn_rewards)
    
    # First subplot: Total Reward vs Turns
    ax[0, 0].cla()  # Clear the first subplot
    ax[0, 0].plot(turn_numbers, total_rewards, marker='o', color='b', label="Total Reward")
    ax[0, 0].set_title("Total Reward vs Turns")
    ax[0, 0].set_xlabel("Turns")
    ax[0, 0].set_ylabel("Total Reward")
    ax[0, 0].grid(True)
    ax[0, 0].legend()

    # Fourth subplot: Turn Reward vs Turn Number
    ax[0, 1].cla()  # Clear the fourth subplot
    ax[0, 1].plot(turn_numbers, turn_rewards, marker='x', color='r', label="Turn Reward")
    ax[0, 1].set_title("Turn Reward vs Turns")
    ax[0, 1].set_xlabel("Turns")
    ax[0, 1].set_ylabel("Turn Reward")
    ax[0, 1].grid(True)
    ax[0, 1].legend()

    ax[0,2].cla()  # Clear the fourth subplot
    ax[0,2].plot(game_numbers, average_game_q_values, marker='x', color='r', label="Game Q-Value")
    ax[0,2].set_title("Average Game Q-Value vs Game Number")
    ax[0,2].set_xlabel("Game Number")
    ax[0,2].set_ylabel("Average Game Q-Value")
    ax[0,2].grid(True)
    ax[0,2].legend()

    # Second subplot: Pie chart for black wins, white wins, and stalemates (only if count > 0)
    total_games = white_wins + black_wins + stalemates
    ax[1, 0].cla()  # Clear the second subplot
    if total_games > 0:
        labels = ['White Wins', 'Black Wins', 'Stalemates']
        colors = ['#FF9999', '#66B2FF', '#99FF99']  # Different colors for each slice
        sizes = [white_wins, black_wins, stalemates]
        ax[1, 0].pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90, colors=colors)
        ax[1, 0].axis('equal')  # Equal aspect ratio ensures the pie chart is circular
        ax[1, 0].set_title("Game Outcome Distribution")

    # Third subplot: Total Reward vs Game Number (without losses scatter)
    ax[1, 1].cla()  # Clear the third subplot
    
    # Third subplot: plot total rewards vs game numbers
    ax[1, 1].plot(game_numbers, game_total_rewards, marker='o', color='g', label="Total Reward Difference per Game")
    ax[1, 1].set_title("Total Reward vs Game Number")
    ax[1, 1].set_xlabel("Game Number")
    ax[1, 1].set_ylabel("Total Reward")
    ax[1, 1].grid(True)
    ax[1, 1].legend()

    ax[1,2].cla()  # Clear the fourth subplot
    ax[1,2].plot(turn_numbers, turn_q_values, marker='x', color='r', label="Turn Q-Value")
    ax[1,2].plot(turn_numbers, stockBestVals, marker='x', color='b', label="Stock Q-Value")
    ax[1,2].set_title("Turn Q-Value vs Turns")
    ax[1,2].set_xlabel("Turns")
    ax[1,2].set_ylabel("Turn Q-Value")
    ax[1,2].grid(True)
    ax[1,2].legend()

    plt.tight_layout()

    # Update the plot
    plt.draw()
    plt.pause(0.1)  # Pause to allow the plot to update



# Function to handle click events
def on_click(event):
    global highlighted_square, highlighted_legal_moves, firstClick, uci_click, turn  # Access global variables

    # Calculate the row and column of the clicked square based on the canvas click
    col = event.x // 80
    row = event.y // 80

    # Convert to chess notation (e.g., "a1", "h8") based on the reversed orientation
    chess_col = chr(col + 97)  # Convert column to letter ('a' to 'h')
    chess_row = 8 - row        # Convert row to number ('1' to '8') based on flipped orientation
    clicked_square = chess_col + str(chess_row)   
    # Convert the file (char) to an index (0-7)
    file_index = ord(chess_col) - 97  # 'a' -> 0, 'b' -> 1, ..., 'h' -> 7
    
    # Convert the rank (char) to an integer (1 -> 0, 2 -> 1, ..., 8 -> 7)
    rank_index = int(chess_row) - 1  # "1" -> 0, "2" -> 1, ..., "8" -> 7
                
    # Use chess.square to convert (file_index, rank_index) to the square index
    square = chess.square(file_index, rank_index)
    

    # Print the chessboard notation (e.g., "a1", "h8") for debugging
    print(f"Clicked on square: {clicked_square}")
    print(f"Square index: {square}")

    # Check if there's a piece on the clicked square
    piece = board.piece_at(chess.square(col, 7 - row))

    # If it's the first click and the user selects a piece
    if firstClick:
        if piece:
            # Get legal moves for the clicked square (if it's a piece)
            highlighted_legal_moves = get_legal_moves(board, clicked_square)
            highlighted_square = (row, col)  # Highlight the selected piece's position

            # Debugging: Check if the highlighted_legal_moves is being populated
            print(f"Highlighted legal moves: {highlighted_legal_moves}")
            uci_click = clicked_square
            print("uci clicked = ", uci_click)
            # After the first click, switch to second click mode
            firstClick = False
        else:
            print("No piece at clicked square. Please select a piece.")
    
    else:
        # If it's the second click, check if the square is a legal move
        print("legal moves array: ", highlighted_legal_moves)
        if square in highlighted_legal_moves:
            print("Legal move found!")
            uci_click += clicked_square
            print("uci clicked = ", uci_click)
            makePlayerMove(uci_click)  # Execute the move 
            firstClick = True   
            uci_click = ""
            highlighted_square = None
            highlighted_legal_moves = []
            turn.set(1)
            # Reset to first click mode after move
        elif piece:
            # Check if the clicked piece is a rook for castling logic
            print(f"Clicked on: {piece} at {clicked_square}")
            if board.has_castling_rights(chess.WHITE):
                if (clicked_square == "h1" or clicked_square == "g1") and uci_click == "e1":
                    makePlayerMove("e1g1")
                    firstClick = True
                    highlighted_square = None
                    highlighted_legal_moves = []
                    turn.set(1)
                    return
                elif (clicked_square == "a1" or clicked_square == "c1")  and uci_click == "e1":
                    makePlayerMove("e1c1")
                    firstClick = True
                    highlighted_square = None
                    highlighted_legal_moves = []
                    turn.set(1)
                    return
                else:
                    print("Clicked on a piece: " + str(piece) + ". At index: " + str(square) + ". Clicked on:" + clicked_square)
                    highlighted_square = None
                    highlighted_legal_moves = []
                    firstClick = True  # Go back to selecting a new piece
                    return
            print("Clicked on a piece: " + str(piece) + ". At index: " + str(square) + ". Clicked on:" + clicked_square)
            highlighted_square = None
            highlighted_legal_moves = []
            firstClick = True  # Go back to selecting a new piece
        else:
            print("Clicked on an invalid square or no piece to move. Clicked on:" + str(square))
            # Optionally, reset the highlighted square and legal moves
            highlighted_square = None
            highlighted_legal_moves = []
            firstClick = True  # Go back to selecting a new piece


    # Redraw the board with the updated highlighted square and legal moves
    canvas.delete("all")  # Clear the canvas
    display_board()        # Redraw the board with updated highlights


# Function to get legal moves for a given square (in UCI notation)
def get_legal_moves(board, clicked_square):
    legal_moves = []
    moves = list(board.legal_moves)  # List of legal moves
    
    for move in moves:
        uci_move = move.uci()  # Get move in UCI notation
        
        # Check if the move involves the clicked square
        if uci_move[:2] == clicked_square:
            # Castling move check (King moves two squares)
            if uci_move == "e1g1" or uci_move == "e1c1" or uci_move == "e8g8" or uci_move == "e8c8":
                destination_square_uci = uci_move[2:4]  # Extract destination square in UCI format
                if destination_square_uci == "g1":
                    destination_square_uci = "f1"
                elif destination_square_uci == "c1":
                    destination_square_uci = "d1"
                elif destination_square_uci == "g8":
                    destination_square_uci = "f8"
                elif destination_square_uci == "c8":
                    destination_square_uci = "d8"
                # This is a castling move, add it to legal_moves and remove from moves
                legal_moves.append(destination_square_uci)  # Add the destination square
                moves.remove(move)  # Remove from moves list
            else:
                # Handle regular moves
                destination_square_uci = uci_move[2:4]  # Extract destination square in UCI format
                file_char = destination_square_uci[0]  # e.g., "e"
                rank_char = destination_square_uci[1]  # e.g., "4"
                
                # Convert the file (char) to an index (0-7)
                file_index = ord(file_char) - 97  # 'a' -> 0, 'b' -> 1, ..., 'h' -> 7
                
                # Convert the rank (char) to an integer (1 -> 0, 2 -> 1, ..., 8 -> 7)
                rank_index = int(rank_char) - 1  # "1" -> 0, "2" -> 1, ..., "8" -> 7
                
                # Use chess.square to convert (file_index, rank_index) to the square index
                destination_square = chess.square(file_index, rank_index)
                
                legal_moves.append(destination_square)  # Add regular move to legal_moves
    
    return legal_moves


# Function to display the board in Tkinter
def display_board():
    global highlighted_legal_moves
    square_size = 80
    for row in range(8):
        for col in range(8):
            # Determine the color of the square
            color = GREEN if (row + col) % 2 == 0 else BEIGE
            canvas.create_rectangle(col * square_size, row * square_size,
                                    (col + 1) * square_size, (row + 1) * square_size,
                                    fill=color)

            # Highlight the selected square if it matches
            if highlighted_square == (row, col):
                canvas.create_rectangle(col * square_size, row * square_size,
                                        (col + 1) * square_size, (row + 1) * square_size,
                                        outline="red", width=3)  # Red border for selected square

            # Highlight the legal moves (blue border)
            if chess.square(col, 7- row) in highlighted_legal_moves:
                print(f"Legal move found: {chess.square(col, 7 - row)}")
                canvas.create_rectangle(col * square_size, row * square_size,
                                        (col + 1) * square_size, (row + 1) * square_size,
                                        outline="blue", width=2)  # Blue border for legal moves

            # Get the piece at the current square
            piece = board.piece_at(chess.square(col, 7 - row))  # Flip y-axis for correct orientation
            if piece:
                piece_char = ""
                if piece.color == chess.WHITE:
                    piece_char = white_pieces.get(piece.piece_type, "")
                else:
                    piece_char = black_pieces.get(piece.piece_type, "")
                canvas.create_text(col * square_size + square_size / 2,
                                   row * square_size + square_size / 2,
                                   text=piece_char, font=("Times New Roman", 36))

            '''
            # Display the square number (0 to 63) in the center of the square
            square_number = chess.square(col, 7 - row)  # Get the square number from (col, row)
            canvas.create_text(col * square_size + square_size / 2,
                               row * square_size + square_size / 2,
                               text=str(square_number), font=("Times New Roman", 12, "bold"), fill="black")
            '''

    # Draw row labels (1-8) on the left side of the board
    for row in range(8):
        canvas.create_text(8 * square_size + 10, row * square_size + square_size / 2,  # Right side positioning
                           text=str(8 - row), font=("Times New Roman", 14))

    # Draw column labels (a-h) at the bottom of the board
    for col in range(8):
        canvas.create_text(col * square_size + square_size / 2,
                           8 * square_size + 10,  # Position below the board
                           text=chr(ord('a') + col), font=("Times New Roman", 14))
        


def makePlayerMove(move_uci):
    global board
    try:
        # Apply the move using UCI notation
        move = chess.Move.from_uci(move_uci)
        piece = board.piece_at(move.from_square)
        # Check for promotion
        if piece and piece.piece_type == chess.PAWN and \
            ((move.to_square < 65 and move.to_square > 55) or (move.to_square < 8 and move.to_square > -1)):  
            # Pawn moved to promotion rank (1st or 8th rank)
            print("promoting")
            
            # Ask for promotion choice using Tkinter dialog
            root = tk.Tk()
            root.withdraw()  # Hide the main window
            promotion_choice = simpledialog.askstring("Pawn Promotion", 
                                                      "Promote to (Q [Queen], R [Rook], B [Bishop], N [Knight]):", 
                                                      parent=root)
            # Map promotion choice to the correct piece type
            piece_map = {'Q': chess.QUEEN, 'R': chess.ROOK, 'B': chess.BISHOP, 'N': chess.KNIGHT}
            
            if promotion_choice in piece_map:  # Validate input
                promotion_piece = piece_map[promotion_choice]  # Get the piece type constant
                print("promotion piece: ", promotion_piece)
                
                # Create the promoted move
                move = chess.Move(move.from_square, move.to_square, promotion=promotion_piece)
                print("new promotion move: ", move)
                print(f"Pawn promoting to {promotion_choice}")
        if move in board.legal_moves:
            board.push(move)
            print(f"Move made: {move_uci}")
            display_board()
        else:
            print("Invalid move!")
    except ValueError:
        print("Invalid UCI format or move.")

# Function to handle a player's move from the console
def make_move(move_uci):
    global board
    try:
        # Apply the move using UCI notation
        move = chess.Move.from_uci(move_uci)
        if move in board.legal_moves:
            board.push(move)
            print(f"Move made: {move_uci}")
            display_board()
        else:
            print("Invalid move!")
    except ValueError:
        print("Invalid UCI format or move.")


def board_to_tensor(board):
    # Create a tensor to represent the board state (12 layers, 8x8 grid)
    tensor = torch.zeros((12, 8, 8), dtype=torch.float32)  # 12 layers (6 piece types for white and black)

    # Iterate over each square on the chessboard (64 squares in total)
    for square in range(64):
        piece = board.piece_at(square)

        if piece:  # If there is a piece at this square
            row, col = divmod(square, 8)  # Get the row and column from the square number

            # Determine the piece type and color
            piece_type = piece.piece_type
            color = piece.color

            # Determine the index in the tensor: for white pieces, use positive values
            # For black pieces, use negative values and the index to determine layer
            layer = piece_type - 1  # piece_type 1 corresponds to layer 0, etc.

            if color == chess.BLACK:
                layer += 6  # Black pieces are in the second half of the tensor (layers 6 to 11)

            # Add the piece's value to the appropriate position in the tensor
            tensor[layer, row, col] = 1  # We are simply marking the presence of a piece, so use a value of 1

    # Flatten the tensor to match the input shape expected by the neural network
    tensor = tensor.view(-1)  # Flatten the 12x8x8 tensor into a 768-length vector
    return tensor

def print_board_tensor():
    tensor = board_to_tensor(board)
    print("Tensor shape:", tensor.shape)
    print(tensor)

rla_agent = ChessRLAI(model=DQN())
turn = tk.IntVar(value=0)

# Function to handle the Player vs RLAI mode
def play_pvrla():
    global stalemates, white_wins, black_wins, game_numbers, total_rewards, turn_numbers, turn_number,  average_game_q_values, turn_q_values, average_q_value
    print("play_pvrla started")
    while not board.is_game_over():
        print("turn before display_board: ", turn.get())
        display_board()
        root.update_idletasks()

        print(" ")
        print("Legal moves: ")
        print(board.legal_moves)

        if turn.get() == 0:  # White Player's Turn
            print("White Player's Turn = ", turn.get())
            canvas.bind("<Button-1>", on_click)  # Bind the event to on_click
            root.wait_variable(turn)  # Wait for the player to make a move
            canvas.unbind("<Button-1>")  # Unbind the event after the move is made
            print("Turn after White's move: ", turn.get())
            if turn.get() == 0:
                print("Turn didn't change after White's move, check on_click.")
            turn.set(1)  # Change to Black's turn after player's move
            print("Turn after setting to Black: ", turn.get()) 
            root.update_idletasks()  # Force UI updates
            root.after(100)  # Add a small delay to ensure the turn is updated
            plot_results()
            print("turn after setting to Black: ", turn.get())

        elif turn.get() == 1:  # Black AI's Turn
            print("Black AI's Turn = ", not turn.get())
            action = rla_agent.find_best_move_with_q_values(board)  # Choose an action based on the state
            print("AI action: ", action)
            legal_moves = list(board.legal_moves)

            # Make sure the action is one of the legal moves
            if action in legal_moves:
                move = action  # Get the corresponding move from legal_moves
                print(f"Black AI plays: {move}")
                make_move(move.uci())  # Apply the move
                display_board()  # Ensure the board is redrawn after the move
                root.update_idletasks()  # Force UI updates
                root.after(20)  # Non-blocking way to add a delay for viewing purposes
                turn_number += 1
                turn_numbers.append(turn_number)
                if len(turn_rewards) == 0:
                    turn_reward = rla_agent.total_reward
                else:
                    turn_reward = ((rla_agent.total_reward)-(total_rewards[-1]))
                print("turn reward: ", turn_reward)
                turn_rewards.append(turn_reward)
                total_rewards.append(rla_agent.total_reward)
                if(average_game_q_values.__len__() <= 1):
                    average_q_value = best_move_q_value
                else:
                    average_q_value += best_move_q_value
                    average_q_value /= 2
                print("average q vals:", average_q_value)
                plot_results()
            else:
                continue
            
            #rla_agent.update_model_after_move(board, move, rla_agent.reward_of_move(move, board))
            
            turn.set(0)  # Change to White's turn after AI's move

        # Introduce a small delay for better viewing of the moves
        root.after(20)  # Non-blocking way to add a delay for viewing purposes
    print("average q vals:", average_q_value)
    game_numbers.append(game_numbers[-1] + 1)
    average_game_q_values.append(average_q_value)
    average_q_value = 0.0


    # Once the game is over, print the result
    print("Game Over!")
    print(f"Result: {board.result()}")


    current_game_total_reward = rla_agent.sum_reward  # Get the total reward for the current game

    if len(game_total_rewards) == 0:
        # For the first game, just append the total reward
        game_total_rewards.append(current_game_total_reward)
    else:
        # For subsequent games, append the difference from the last game
        last_game_total_reward = game_total_rewards[-1]  # Get the last appended total reward
        reward_difference = current_game_total_reward - last_game_total_reward  # Calculate the difference
        game_total_rewards.append(reward_difference)  # Store the difference in total rewards

    result = board.result()
    if result == "1-0":  # White wins
        white_wins += 1
        rla_agent.total_reward -= 100
        print("Player wins!")
    elif result == "0-1":  # Black wins
        black_wins += 1
        print("AI wins!")
    else:
        stalemates += 1
        rla_agent.total_reward -= 30
        print("It's a draw!")

    print("white wins: ", white_wins)
    print("black wins: ", black_wins)
    print("stalemates: ", stalemates)


    reset_game()
    plot_results()
    start_game()

def reset_game():
    global highlighted_square, highlighted_legal_moves, firstClick, uci_click, turn, board, canvas
    
    # Reset global variables
    highlighted_square = None
    highlighted_legal_moves = []
    firstClick = True
    uci_click = ""
    
    # Set turn to white (0 for white's turn, 1 for black's turn)
    turn.set(0)  # White's turn (set to 0 for white, 1 for black)

    # Reset the chessboard to the starting position
    board = chess.Board()
    
    # Redraw the board
    canvas.delete("all")  # Clear the canvas
    display_board()        # Redraw the board with updated highlights (assuming you have a display_board function)

    # Print a statement for debugging purposes
    print("Game has been reset. It is now White's turn.")



# Main function to start the game based on the selected mode
def start_game():
    display_board()
    print("Starting Player vs RLAI mode...")
    play_pvrla()
    start_game()

# Call the function to update model first

#trainer = ModelTrainer()

# Call pretrain_and_save_model before starting the game
#trainer.pretrain_and_save_model()

# Start the game
start_game()